# ğŸš€ Quick Start Guide - Qwen3 Integration

## âœ… Báº¡n Ä‘Ã£ hoÃ n thÃ nh tÃ­ch há»£p Qwen3!

### ğŸ“ Files Ä‘Ã£ táº¡o:

```
app/
â”œâ”€â”€ services/
â”‚   â””â”€â”€ qwen_service.py        âœ… Service giao tiáº¿p vá»›i vLLM
â”œâ”€â”€ api/
â”‚   â””â”€â”€ qwen.py                âœ… API routes cho Qwen
â””â”€â”€ core/
    â””â”€â”€ config.py              âœ… Config Ä‘Ã£ update

test_qwen.py                   âœ… Test script
```

---

## ğŸ¯ CÃ¡ch sá»­ dá»¥ng

### BÆ°á»›c 1: Äáº£m báº£o vLLM Ä‘ang cháº¡y

```powershell
# Kiá»ƒm tra vLLM cÃ³ cháº¡y khÃ´ng
curl http://localhost:8000/v1/models
```

Náº¿u chÆ°a cháº¡y, start vLLM:

```powershell
vllm serve Qwen/Qwen3-0.6B --port 8000
```

### BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng FastAPI

```powershell
# Náº¿u chÆ°a cháº¡y
python main.py

# Hoáº·c
uvicorn main:app --reload
```

### BÆ°á»›c 3: Test Qwen Endpoints

#### **Option 1: DÃ¹ng test script (Recommended)**

```powershell
python test_qwen.py
```

Test script sáº½ cháº¡y:

- âœ“ Health check
- âœ“ Model info
- âœ“ Non-streaming chat
- âœ“ Streaming chat
- âœ“ Custom system prompts

#### **Option 2: DÃ¹ng Interactive API Docs**

Má»Ÿ browser: http://localhost:8000/docs

Báº¡n sáº½ tháº¥y group má»›i: **"Qwen Chat"** vá»›i 4 endpoints:

- `GET /qwen/health` - Health check
- `GET /qwen/info` - Model information
- `POST /qwen/chat` - Non-streaming chat
- `POST /qwen/chat/stream` - Streaming chat

#### **Option 3: DÃ¹ng curl**

```powershell
# 1. Health check
curl http://localhost:8000/qwen/health

# 2. Model info
curl http://localhost:8000/qwen/info

# 3. Simple chat
curl -X POST "http://localhost:8000/qwen/chat" ^
  -H "Content-Type: application/json" ^
  -d "{\"message\": \"What is RAG in AI?\"}"

# 4. Chat with system prompt
curl -X POST "http://localhost:8000/qwen/chat" ^
  -H "Content-Type: application/json" ^
  -d "{\"message\": \"Write a hello world in Python\", \"system_prompt\": \"You are a Python expert\"}"
```

#### **Option 4: DÃ¹ng Python requests**

```python
import requests

# Simple chat
response = requests.post(
    "http://localhost:8000/qwen/chat",
    json={
        "message": "Explain FastAPI briefly",
        "temperature": 0.7
    }
)

print(response.json()["response"])
```

---

## ğŸ“– API Endpoints Chi Tiáº¿t

### 1. `GET /qwen/health`

**Má»¥c Ä‘Ã­ch:** Kiá»ƒm tra Qwen service cÃ³ hoáº¡t Ä‘á»™ng khÃ´ng

**Response:**

```json
{
  "status": "healthy",
  "service": "qwen-vllm",
  "model": "Qwen/Qwen3-0.6B",
  "server_url": "http://localhost:8000"
}
```

### 2. `GET /qwen/info`

**Má»¥c Ä‘Ã­ch:** Láº¥y thÃ´ng tin vá» Qwen model

**Response:**

```json
{
  "model_name": "Qwen/Qwen3-0.6B",
  "model_size": "600M parameters",
  "context_length": "32,768 tokens",
  "features": [...],
  "use_cases": [...]
}
```

### 3. `POST /qwen/chat`

**Má»¥c Ä‘Ã­ch:** Chat non-streaming vá»›i Qwen3

**Request:**

```json
{
  "message": "What is FastAPI?",
  "temperature": 0.7, // Optional: 0.0-2.0
  "max_tokens": 1000, // Optional
  "system_prompt": "You are..." // Optional
}
```

**Response:**

```json
{
  "response": "FastAPI is a modern...",
  "conversation_id": "qwen_conv_abc123",
  "model": "Qwen/Qwen3-0.6B",
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 50,
    "total_tokens": 60
  }
}
```

### 4. `POST /qwen/chat/stream`

**Má»¥c Ä‘Ã­ch:** Chat vá»›i streaming response (real-time)

**Request:** Giá»‘ng nhÆ° `/qwen/chat`

**Response:** Server-Sent Events (SSE)

```
data: {"chunk": "Fast", "done": false, "conversation_id": "..."}

data: {"chunk": "API", "done": false, "conversation_id": "..."}

data: {"chunk": "", "done": true, "conversation_id": "..."}
```

---

## ğŸ’¡ Use Cases & Examples

### 1. Simple Q&A

```json
{
  "message": "What is RAG in AI?",
  "temperature": 0.7
}
```

### 2. Code Generation

```json
{
  "message": "Write a Python function to calculate fibonacci",
  "system_prompt": "You are an expert Python programmer",
  "temperature": 0.3
}
```

### 3. Text Summarization

```json
{
  "message": "Summarize this text: [long text here]",
  "temperature": 0.5,
  "max_tokens": 500
}
```

### 4. Translation

```json
{
  "message": "Translate to Vietnamese: Hello, how are you?",
  "system_prompt": "You are a professional translator",
  "temperature": 0.3
}
```

### 5. Creative Writing

```json
{
  "message": "Write a short story about AI",
  "system_prompt": "You are a creative writer",
  "temperature": 1.5
}
```

---

## ğŸ”§ Troubleshooting

### âŒ Error: "Qwen vLLM server is not accessible"

**Solutions:**

1. Check vLLM cÃ³ cháº¡y khÃ´ng:

   ```powershell
   curl http://localhost:8000/v1/models
   ```

2. Start vLLM náº¿u chÆ°a cháº¡y:

   ```powershell
   vllm serve Qwen/Qwen3-0.6B --port 8000
   ```

3. Check port conflict (port 8000 bá»‹ chiáº¿m):
   ```powershell
   netstat -ano | findstr :8000
   ```

### âŒ Error: "Connection refused"

**Solutions:**

1. Äáº£m báº£o FastAPI Ä‘ang cháº¡y:

   ```powershell
   python main.py
   ```

2. Check Ä‘Ãºng port chÆ°a (FastAPI default: 8000, nhÆ°ng cÃ³ thá»ƒ Ä‘á»•i trong .env)

### âŒ Response quÃ¡ cháº­m

**Solutions:**

1. Giáº£m `max_tokens` xuá»‘ng (vd: 500)
2. TÄƒng timeout trong config
3. Check CPU/Memory usage
4. Náº¿u cÃ³ GPU, Ä‘áº£m báº£o vLLM Ä‘ang dÃ¹ng GPU

### âŒ Response khÃ´ng Ä‘Ãºng Ã½

**Solutions:**

1. Thá»­ thay Ä‘á»•i `temperature`:
   - Tháº¥p hÆ¡n (0.3) = deterministic
   - Cao hÆ¡n (1.5) = creative
2. Thá»­ thÃªm `system_prompt` cá»¥ thá»ƒ hÆ¡n
3. Refine cÃ¢u há»i rÃµ rÃ ng hÆ¡n

---

## ğŸ“Š So sÃ¡nh Gemini vs Qwen3

| Feature      | Gemini (Cloud)      | Qwen3 (Local)         |
| ------------ | ------------------- | --------------------- |
| **Location** | Cloud API           | Local (vLLM)          |
| **Size**     | Large (billions)    | 600M params           |
| **Speed**    | Fast (Google infra) | Depends on hardware   |
| **Cost**     | API quota/paid      | Free (after download) |
| **Privacy**  | Data sent to Google | 100% local, private   |
| **Internet** | Required            | Not required          |
| **Context**  | 1M tokens           | 32K tokens            |
| **Quality**  | Excellent           | Good for size         |
| **Use Case** | Production          | Learning, RAG, local  |

---

## ğŸ“ BÆ°á»›c tiáº¿p theo

### âœ… HoÃ n thÃ nh:

- [x] Qwen3 service
- [x] API endpoints
- [x] Streaming support
- [x] Health checks
- [x] Test script

### ğŸ“… Tiáº¿p theo há»c:

1. **Vector Databases** (Week 3-4)

   - ChromaDB integration
   - Document embeddings
   - Semantic search

2. **RAG vá»›i Qwen3** (Week 5-6)

   - Document ingestion
   - Retrieval pipeline
   - Context-aware generation

3. **LangChain** (Week 7-8)

   - Chains vá»›i Qwen3
   - Memory management
   - Agent creation

4. **Advanced RAG** (Week 9-12)
   - Agentic RAG
   - LangGraph workflows
   - Multi-agent systems

---

## ğŸ“ Notes

### Qwen3 advantages cho learning:

1. âœ… **Local = Privacy** - KhÃ´ng lo data leak
2. âœ… **Free** - KhÃ´ng tá»‘n API cost
3. âœ… **Fast iteration** - Test nhanh, khÃ´ng limit
4. âœ… **Good for RAG** - Context window 32K tokens
5. âœ… **Educational** - Hiá»ƒu Ä‘Æ°á»£c flow cá»§a LLM

### Khi nÃ o dÃ¹ng Gemini vs Qwen3:

- **Gemini**: Production, cáº§n quality cao, internet stable
- **Qwen3**: Learning, testing, RAG experiments, offline

---

## ğŸ‰ ChÃºc má»«ng!

Báº¡n Ä‘Ã£ tÃ­ch há»£p thÃ nh cÃ´ng Qwen3-0.6B vÃ o project!

**Next steps:**

1. Cháº¡y `python test_qwen.py` Ä‘á»ƒ verify
2. Thá»­ cÃ¡c use cases khÃ¡c nhau
3. Compare response quality giá»¯a Gemini vÃ  Qwen3
4. Chuáº©n bá»‹ há»c RAG vá»›i Qwen3!

**Happy coding!** ğŸš€
