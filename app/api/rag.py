"""
RAG (Retrieval-Augmented Generation) API
API ƒë·ªÉ h·ªèi ƒë√°p d·ª±a tr√™n documents ƒë√£ upload

=============================================================================
                    LEARNING NOTES - H·ªåC V·ªÄ RAG
=============================================================================

1. RAG L√Ä G√å? (What is RAG?)
   ==========================
   
   RAG = Retrieval-Augmented Generation
   - Retrieval: T√¨m ki·∫øm th√¥ng tin li√™n quan t·ª´ documents
   - Augmented: TƒÉng c∆∞·ªùng, b·ªï sung
   - Generation: AI sinh ra c√¢u tr·∫£ l·ªùi
   
   Workflow:
   User question ‚Üí Search docs ‚Üí AI reads context ‚Üí Generate answer
   
   T·∫°i sao c·∫ßn RAG?
   - AI thu·∫ßn: Ch·ªâ bi·∫øt data ƒë∆∞·ª£c train (outdated, kh√¥ng c√≥ info ri√™ng b·∫°n)
   - AI + RAG: C√≥ th·ªÉ answer d·ª±a tr√™n YOUR documents (up-to-date, specific)
   - No hallucination: AI kh√¥ng b·ªãa, v√¨ c√≥ context t·ª´ docs

2. SO S√ÅNH: CHAT THU·∫¶N vs RAG
   ===========================
   
   A. Chat Thu·∫ßn (chat.py, qwen.py):
      - User: "Giyu Tomioka l√† ai?"
      - AI: "T√¥i kh√¥ng c√≥ th√¥ng tin c·ª• th·ªÉ..." (general knowledge only)
      - Kh√¥ng access documents
   
   B. RAG (rag.py - file n√†y):
      - User: "Giyu Tomioka l√† ai?"
      - System: Search trong docs ‚Üí Find chunks v·ªÅ Giyu
      - AI: "Theo t√†i li·ªáu, Giyu Tomioka l√†..." (based on YOUR docs)
      - Accurate + c√≥ source citations

3. RAG WORKFLOW CHI TI·∫æT
   ======================
   
   Step 1: Embed Question
   - Input: "C√°ch c√†i ƒë·∫∑t Python?"
   - Process: embedding_service.embed_text(question)
   - Output: [0.234, -0.567, ..., 0.890] (768d vector)
   
   Step 2: Search Similar Chunks
   - Input: question_embedding
   - Process: vector_db.search(embedding, n_results=5)
   - Output: Top 5 most similar chunks
   
   Step 3: Build Context
   - Input: List of chunks
   - Process: Combine chunks v·ªõi separators
   - Output: Long text v·ªõi all relevant info
   
   Step 4: Create Prompt
   - Template: "Context: {chunks}\n\nQuestion: {q}\n\nAnswer:"
   - AI s·∫Ω ƒë·ªçc context tr∆∞·ªõc khi tr·∫£ l·ªùi
   
   Step 5: Generate Answer
   - Input: Prompt v·ªõi context
   - Process: gemini.chat(prompt)
   - Output: Answer based on context
   
   Step 6: Return Response
   - Answer + Sources + Context used
   - User c√≥ th·ªÉ verify t·ª´ sources

4. PROMPT ENGINEERING
   ===================
   
   Good prompt structure:
   ```
   Based on the following context from documents, answer the question.
   
   Context:
   [Document chunks here]
   
   Question: [User's question]
   
   Instructions:
   - Answer based ONLY on the context provided
   - If context doesn't have info, say "I don't have information about this"
   - Be specific and cite information
   - Use clear, concise language
   ```
   
   T·∫°i sao structure n√†y t·ªët?
   - Clear instructions: AI bi·∫øt ph·∫£i l√†m g√¨
   - Context first: AI ƒë·ªçc context tr∆∞·ªõc
   - Grounding: "based ONLY on context" ‚Üí no hallucination
   - Fallback: N·∫øu kh√¥ng bi·∫øt, th·ª´a nh·∫≠n thay v√¨ b·ªãa

5. N_RESULTS - S·ªê CHUNKS C·∫¶N L·∫§Y
   ==============================
   
   Trade-offs:
   
   Low n_results (1-3):
   - Pros: Fast, focused, less tokens cost
   - Cons: C√≥ th·ªÉ miss important info
   - Use: Simple questions, specific topics
   
   Medium n_results (5-10):
   - Pros: Balanced, good coverage
   - Cons: More tokens, slightly slower
   - Use: General questions (recommended default)
   
   High n_results (15-20):
   - Pros: Maximum context, comprehensive
   - Cons: Expensive tokens, slower, may confuse AI
   - Use: Complex questions, need full picture
   
   Gemini limits:
   - Max context: 30,720 tokens (~100,000 characters)
   - Each chunk: ~500 words (~3,000 chars)
   - Safe: 10-15 chunks max

=============================================================================
"""

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, List, Dict, Any, AsyncIterator
import time
import json

from app.services.embedding_service import EmbeddingService
from app.services.vector_db_service import VectorDBService
from app.services.gemini_service import GeminiService

# Create router v·ªõi prefix v√† tags
# LEARNING: prefix="/api/rag" ‚Üí all endpoints start v·ªõi /api/rag
# tags=["rag"] ‚Üí group trong API docs (Swagger UI)
router = APIRouter(prefix="/api/rag", tags=["rag"])

# Initialize services
# LEARNING: Reuse services ƒë√£ c√≥, kh√¥ng t·∫°o m·ªõi
# Singleton pattern: One instance shared across requests
embedding_service = EmbeddingService()
vector_db = VectorDBService()
gemini_service = GeminiService()



class RAGQueryRequest(BaseModel):
    """
    Request body cho RAG query endpoint
    
    LEARNING - PYDANTIC MODELS:
    ==========================
    - Automatic validation: FastAPI t·ª± ki·ªÉm tra types
    - Documentation: Swagger UI show schema
    - Type hints: IDE autocomplete
    """
    question: str  # Required: User's question (c√¢u h·ªèi c·ªßa user)
    n_results: int = 5  # Optional: S·ªë chunks mu·ªën l·∫•y (default: 5)
    document_id: Optional[str] = None  # Optional: Search trong doc c·ª• th·ªÉ
    include_context: bool = False  # Optional: Return context used hay kh√¥ng


class RAGQueryResponse(BaseModel):
    """
    Response structure cho RAG query
    
    LEARNING: Clear response structure gi√∫p frontend d·ªÖ handle
    """
    success: bool
    question: str
    answer: str
    sources: List[Dict[str, Any]]  # List of chunks used
    context_used: Optional[str]  # Full context (if requested)
    metadata: Dict[str, Any]  # Stats: chunks_count, processing_time, etc.


@router.post("/query")
async def rag_query(request: RAGQueryRequest) -> RAGQueryResponse:
    """
    RAG Query Endpoint - H·ªèi ƒë√°p d·ª±a tr√™n documents ƒë√£ upload
    
    =============================================================================
    LEARNING - RAG QUERY IMPLEMENTATION
    =============================================================================
    
    Workflow ho√†n ch·ªânh:
    1. Validate input (FastAPI t·ª± ƒë·ªông)
    2. Embed question ‚Üí vector
    3. Search similar chunks trong vector DB
    4. Build context t·ª´ chunks
    5. Create prompt v·ªõi context
    6. Call Gemini ƒë·ªÉ generate answer
    7. Format v√† return response
    
    Error handling:
    - No documents: "Please upload documents first"
    - No matches: "No relevant information found"
    - Gemini error: "AI service error"
    
    Args:
        request: RAGQueryRequest object v·ªõi question, n_results, document_id
    
    Returns:
        RAGQueryResponse v·ªõi answer, sources, metadata
    
    Example:
        POST /api/rag/query
        {
            "question": "Giyu Tomioka l√† ai?",
            "n_results": 5,
            "include_context": true
        }
        
        Response:
        {
            "success": true,
            "answer": "Giyu Tomioka l√†...",
            "sources": [...],
            "metadata": {"chunks_used": 5, "time": 2.5}
        }
    """
    try:
        start_time = time.time()
        
        print(f"\n{'='*80}")
        print(f"üîç RAG QUERY STARTED")
        print(f"{'='*80}")
        print(f"Question: {request.question}")
        print(f"N_results: {request.n_results}")
        if request.document_id:
            print(f"Filtering by document: {request.document_id}")
        
        # =====================================================================
        # STEP 1: VALIDATE - Ki·ªÉm tra c√≥ documents trong DB kh√¥ng
        # =====================================================================
        # LEARNING: Check tr∆∞·ªõc khi process ƒë·ªÉ avoid wasted computation
        print(f"\nüìä Step 1: Checking vector database...")
        
        stats = vector_db.get_collection_stats()
        total_chunks = stats.get('total_chunks', 0)
        
        if total_chunks == 0:
            print("‚ö†Ô∏è  No documents found in database!")
            raise HTTPException(
                status_code=404,
                detail="No documents found. Please upload and embed documents first."
            )
        
        print(f"‚úÖ Found {total_chunks} chunks in database")
        
        # =====================================================================
        # STEP 2: EMBED QUESTION - Chuy·ªÉn c√¢u h·ªèi th√†nh vector
        # =====================================================================
        # LEARNING: Question ph·∫£i c√πng format v·ªõi document embeddings
        # C√πng model (text-embedding-004), c√πng dimensions (768)
        print(f"\nüéØ Step 2: Embedding question...")
        
        question_embedding = embedding_service.embed_text(request.question)
        
        print(f"‚úÖ Question embedded to {len(question_embedding)}d vector")
        print(f"   Sample values: [{question_embedding[0]:.4f}, {question_embedding[1]:.4f}, ...]")
        
        # =====================================================================
        # STEP 3: SEARCH - T√¨m chunks t∆∞∆°ng t·ª± trong vector DB
        # =====================================================================
        # LEARNING: Similarity search l√† core c·ªßa RAG
        # ChromaDB compare question_embedding v·ªõi all stored embeddings
        # Return top-k most similar (lowest distance = highest similarity)
        print(f"\nüîç Step 3: Searching for similar chunks...")
        
        # Prepare metadata filter n·∫øu c√≥
        filter_metadata = None
        if request.document_id:
            filter_metadata = {"document_id": request.document_id}
            print(f"   Filtering by: {filter_metadata}")
        
        # Search trong vector DB
        search_results = vector_db.search(
            query_embedding=question_embedding,
            n_results=request.n_results,
            filter_metadata=filter_metadata
        )
        
        # Check if any results found
        if search_results['count'] == 0:
            print("‚ö†Ô∏è  No relevant chunks found!")
            raise HTTPException(
                status_code=404,
                detail="No relevant information found for your question."
            )
        
        print(f"‚úÖ Found {search_results['count']} relevant chunks")
        
        # Log top matches v·ªõi similarity scores
        print(f"\n   Top matches:")
        for i, result in enumerate(search_results['results'][:3], 1):
            distance = result.get('distance', 0)
            similarity = 1 - distance  # Convert distance to similarity
            text_preview = result['text'][:100] + "..."
            print(f"   {i}. Similarity: {similarity:.3f} | {text_preview}")
        
        # =====================================================================
        # STEP 4: BUILD CONTEXT - K·∫øt h·ª£p chunks th√†nh context text
        # =====================================================================
        # LEARNING: Context structure affects AI's answer quality
        # Good structure: Clear separators, numbered chunks, source info
        print(f"\nüìù Step 4: Building context from chunks...")
        
        context_parts = []
        sources = []
        
        for i, result in enumerate(search_results['results'], 1):
            # Format: [Source 1] text...
            # LEARNING: Numbering helps AI reference specific sources
            chunk_text = result['text']
            metadata = result.get('metadata', {})
            
            # Add source info
            source_info = f"[Source {i}]"
            if metadata.get('filename'):
                source_info += f" From: {metadata['filename']}"
            if metadata.get('chunk_index') is not None:
                source_info += f" (Chunk {metadata['chunk_index']})"
            
            # Combine: [Source 1] From: file.pdf (Chunk 5)
            # Text content here...
            context_parts.append(f"{source_info}\n{chunk_text}")
            
            # Prepare sources for response
            sources.append({
                "chunk_id": result['id'],
                "text": chunk_text,
                "text_preview": chunk_text[:200] + "..." if len(chunk_text) > 200 else chunk_text,
                "distance": result.get('distance', 0),
                "similarity": 1 - result.get('distance', 0),
                "metadata": metadata
            })
        
        # Join v·ªõi separator
        # LEARNING: "\n\n---\n\n" creates clear visual separation
        context = "\n\n---\n\n".join(context_parts)
        
        context_length = len(context)
        print(f"‚úÖ Context built: {context_length} characters from {len(context_parts)} chunks")
        
        # =====================================================================
        # STEP 5: CREATE PROMPT - T·∫°o prompt cho Gemini
        # =====================================================================
        # LEARNING: Prompt engineering is critical for good answers
        # Structure: Instructions ‚Üí Context ‚Üí Question ‚Üí Output format
        print(f"\nüí≠ Step 5: Creating prompt for Gemini...")
        
        prompt = f"""You are a helpful AI assistant. Answer the question based ONLY on the context provided below.

CONTEXT FROM DOCUMENTS:
{context}

QUESTION: {request.question}

INSTRUCTIONS:
- Answer in the same language as the question (Vietnamese or English)
- Base your answer ONLY on the information in the context above
- If the context doesn't contain relevant information, say "I don't have enough information to answer this question based on the provided documents."
- Use clear and concise language
- If multiple sources say the same thing, mention that for credibility

ANSWER:"""
        
        prompt_length = len(prompt)
        print(f"‚úÖ Prompt created: {prompt_length} characters")
        print(f"   Context: {context_length} chars | Question: {len(request.question)} chars")
        
        # =====================================================================
        # STEP 6: GENERATE ANSWER - Call Gemini API
        # =====================================================================
        # LEARNING: This is where AI magic happens
        # Gemini reads context + question ‚Üí generates grounded answer
        print(f"\nü§ñ Step 6: Calling Gemini to generate answer...")
        
        try:
            # Call Gemini service
            # LEARNING: gemini_service.generate_response() handles API call, retries, errors
            result = await gemini_service.generate_response(
                message=prompt,
                temperature=0.7  # Lower temperature for factual answers
            )
            
            answer = result['response']
            answer_length = len(answer)
            print(f"‚úÖ Answer generated: {answer_length} characters")
            print(f"   Preview: {answer[:150]}...")
            
        except Exception as e:
            print(f"‚ùå Gemini API error: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"AI service error: {str(e)}"
            )
        
        # =====================================================================
        # STEP 7: FORMAT RESPONSE - Chu·∫©n b·ªã response cho client
        # =====================================================================
        print(f"\nüì¶ Step 7: Formatting response...")
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # Metadata v·ªõi stats
        metadata = {
            "chunks_used": len(sources),
            "total_chunks_available": total_chunks,
            "context_length": context_length,
            "answer_length": answer_length,
            "processing_time_seconds": round(processing_time, 2),
            "model": "gemini-2.0-flash-exp",
            "embedding_model": "text-embedding-004"
        }
        
        response = RAGQueryResponse(
            success=True,
            question=request.question,
            answer=answer,
            sources=sources,
            context_used=context if request.include_context else None,
            metadata=metadata
        )
        
        print(f"\n{'='*80}")
        print(f"‚úÖ RAG QUERY COMPLETED")
        print(f"{'='*80}")
        print(f"Processing time: {processing_time:.2f}s")
        print(f"Chunks used: {len(sources)}")
        print(f"Answer length: {answer_length} characters")
        print(f"\n")
        
        return response
        
    except HTTPException:
        # Re-raise HTTP exceptions (already formatted)
        raise
        
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {str(e)}")
        import traceback
        traceback.print_exc()
        
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}"
        )


@router.post("/query/stream")
async def rag_query_stream(request: RAGQueryRequest):
    """
    RAG Query Streaming Endpoint - Stream answer v·ªõi sources (like chat UI)
    
    =============================================================================
    LEARNING - STREAMING RAG FOR UI
    =============================================================================
    
    Why streaming?
    - Better UX: User sees progress immediately (sources ‚Üí answer streaming)
    - Like chat UI: Show sources as "thinking", then stream answer
    - Engagement: User kh√¥ng ph·∫£i ƒë·ª£i 2-3s cho full response
    
    Stream format (Server-Sent Events):
    1. Event "sources": Show retrieved documents (like thinking phase)
       data: {"type":"sources","chunks":[...],"count":5}
    
    2. Event "answer": Stream answer chunks (like chat streaming)
       data: {"type":"answer","chunk":"Giyu Tomioka..."}
       data: {"type":"answer","chunk":" l√† Th·ªßy Tr·ª•..."}
    
    3. Event "done": Final metadata
       data: {"type":"done","metadata":{...},"done":true}
    
    UI Integration:
    - Phase 1: Show sources (document cards, nh∆∞ thinking)
    - Phase 2: Stream answer text (nh∆∞ chat streaming)
    - Phase 3: Show metadata (time, chunks used)
    
    Example usage:
        const eventSource = new EventSource('/api/rag/query/stream?question=...');
        eventSource.onmessage = (e) => {
            const data = JSON.parse(e.data);
            if (data.type === 'sources') {
                // Show document cards
            } else if (data.type === 'answer') {
                // Append chunk to answer
            } else if (data.type === 'done') {
                // Show metadata, close stream
            }
        };
    """
    
    async def generate_stream() -> AsyncIterator[str]:
        """
        Generator function for SSE streaming
        
        LEARNING: AsyncIterator[str] = async generator
        Yield SSE format: "data: {json}\n\n"
        """
        try:
            start_time = time.time()
            
            print(f"\n{'='*80}")
            print(f"üîç RAG STREAMING QUERY STARTED")
            print(f"{'='*80}")
            print(f"Question: {request.question}")
            
            # ================================================================
            # PHASE 1: RETRIEVE & SEND SOURCES (like "thinking")
            # ================================================================
            print(f"\nüìä Phase 1: Retrieving sources...")
            
            # Step 1: Check DB
            stats = vector_db.get_collection_stats()
            total_chunks = stats.get('total_chunks', 0)
            
            if total_chunks == 0:
                error_data = {
                    "type": "error",
                    "error": "No documents found. Please upload documents first.",
                    "done": True
                }
                yield f"data: {json.dumps(error_data)}\n\n"
                return
            
            # Step 2: Embed question
            question_embedding = embedding_service.embed_text(request.question)
            print(f"‚úÖ Question embedded")
            
            # Step 3: Search chunks
            filter_metadata = None
            if request.document_id:
                filter_metadata = {"document_id": request.document_id}
            
            search_results = vector_db.search(
                query_embedding=question_embedding,
                n_results=request.n_results,
                filter_metadata=filter_metadata
            )
            
            if search_results['count'] == 0:
                error_data = {
                    "type": "error",
                    "error": "No relevant information found.",
                    "done": True
                }
                yield f"data: {json.dumps(error_data)}\n\n"
                return
            
            print(f"‚úÖ Found {search_results['count']} chunks")
            
            # Step 4: Prepare sources
            sources = []
            context_parts = []
            
            for i, result in enumerate(search_results['results'], 1):
                chunk_text = result['text']
                metadata = result.get('metadata', {})
                
                # Source info
                source_info = f"[Source {i}]"
                if metadata.get('filename'):
                    source_info += f" From: {metadata['filename']}"
                if metadata.get('chunk_index') is not None:
                    source_info += f" (Chunk {metadata['chunk_index']})"
                
                context_parts.append(f"{source_info}\n{chunk_text}")
                
                # Prepare source for UI
                sources.append({
                    "chunk_id": result['id'],
                    "text": chunk_text,
                    "text_preview": chunk_text[:200] + "..." if len(chunk_text) > 200 else chunk_text,
                    "distance": result.get('distance', 0),
                    "similarity": 1 - result.get('distance', 0),
                    "metadata": metadata,
                    "source_number": i
                })
            
            context = "\n\n---\n\n".join(context_parts)
            context_length = len(context)
            
            # Send sources event (like "thinking" phase)
            # LEARNING: UI shows n√†y nh∆∞ document cards, gi·ªëng thinking mode
            sources_event = {
                "type": "sources",
                "chunks": sources,
                "count": len(sources),
                "total_chunks_available": total_chunks
            }
            yield f"data: {json.dumps(sources_event, ensure_ascii=False)}\n\n"
            print(f"üì§ Sent sources to UI")
            
            # ================================================================
            # PHASE 2: GENERATE & STREAM ANSWER (like chat streaming)
            # ================================================================
            print(f"\nüí≠ Phase 2: Generating answer...")
            
            # Create prompt
            prompt = f"""You are a helpful AI assistant. Answer the question based ONLY on the context provided below.

CONTEXT FROM DOCUMENTS:
{context}

QUESTION: {request.question}

INSTRUCTIONS:
- Answer in the same language as the question (Vietnamese or English)
- Base your answer ONLY on the information in the context above
- If the context doesn't contain relevant information, say "I don't have enough information to answer this question based on the provided documents."
- Be specific and cite which source ([Source 1], [Source 2], etc.) supports your answer
- Use clear and concise language
- If multiple sources say the same thing, mention that for credibility

ANSWER:"""
            
            print(f"ü§ñ Calling Gemini to stream answer...")
            
            # Stream answer from Gemini
            # LEARNING: generate_stream_response() streams chunks
            answer_text = ""
            
            async for chunk_data in gemini_service.generate_stream_response(
                message=prompt,
                temperature=0.7,
                thinking_budget=0  # No thinking for RAG
            ):
                chunk_type = chunk_data.get('type')
                chunk_text = chunk_data.get('chunk', '')
                
                # Only stream answer chunks (not thoughts, not done)
                if chunk_type == 'answer' and chunk_text:
                    answer_text += chunk_text
                    
                    # Send answer chunk to UI
                    answer_event = {
                        "type": "answer",
                        "chunk": chunk_text,
                        "done": False
                    }
                    yield f"data: {json.dumps(answer_event, ensure_ascii=False)}\n\n"
                
                # Handle errors
                elif chunk_type == 'error':
                    error_event = {
                        "type": "error",
                        "error": chunk_data.get('error', 'Unknown error'),
                        "done": True
                    }
                    yield f"data: {json.dumps(error_event)}\n\n"
                    return
            
            # ================================================================
            # PHASE 3: SEND FINAL METADATA
            # ================================================================
            end_time = time.time()
            processing_time = end_time - start_time
            
            print(f"‚úÖ Answer streamed: {len(answer_text)} characters")
            print(f"‚è±Ô∏è  Total time: {processing_time:.2f}s")
            
            # Send done event v·ªõi metadata
            done_event = {
                "type": "done",
                "done": True,
                "metadata": {
                    "chunks_used": len(sources),
                    "total_chunks_available": total_chunks,
                    "context_length": context_length,
                    "answer_length": len(answer_text),
                    "processing_time_seconds": round(processing_time, 2),
                    "model": "gemini-2.0-flash-exp",
                    "embedding_model": "text-embedding-004"
                }
            }
            yield f"data: {json.dumps(done_event)}\n\n"
            
            print(f"\n{'='*80}")
            print(f"‚úÖ RAG STREAMING COMPLETED")
            print(f"{'='*80}\n")
            
        except Exception as e:
            print(f"\n‚ùå Streaming error: {str(e)}")
            import traceback
            traceback.print_exc()
            
            error_event = {
                "type": "error",
                "error": str(e),
                "done": True
            }
            yield f"data: {json.dumps(error_event)}\n\n"
    
    # Return SSE response
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.get("/stats")
async def get_rag_stats():
    """
    Get RAG System Statistics - Xem tr·∫°ng th√°i h·ªá th·ªëng
    
    =============================================================================
    LEARNING - STATS ENDPOINT
    =============================================================================
    
    Why stats endpoint?
    - Monitor system health: C√≥ documents ch∆∞a? DB ho·∫°t ƒë·ªông?
    - Debug: N·∫øu RAG query fail, check stats first
    - UI: Show "Ready" badge n·∫øu c√≥ data, "Upload documents" n·∫øu empty
    
    Returns:
    - ready: boolean - System ready for queries?
    - total_documents: int - Number of uploaded documents
    - total_chunks: int - Number of embedded chunks
    - status: str - "ready" | "no_data" | "error"
    - message: str - Human-readable status
    
    Example response:
    {
        "ready": true,
        "total_documents": 3,
        "total_chunks": 150,
        "status": "ready",
        "message": "RAG system ready with 3 documents and 150 chunks"
    }
    """
    try:
        print(f"\nüìä Checking RAG system stats...")
        
        # Get collection stats t·ª´ vector DB
        # LEARNING: ChromaDB stores stats v·ªÅ documents v√† chunks
        stats = vector_db.get_collection_stats()
        total_chunks = stats.get('total_chunks', 0)
        
        # Get list of documents
        # LEARNING: Each document c√≥ th·ªÉ c√≥ nhi·ªÅu chunks
        documents = vector_db.list_all_documents()
        total_documents = len(documents)
        
        # Determine system status
        if total_chunks > 0:
            ready = True
            status = "ready"
            message = f"RAG system ready with {total_documents} documents and {total_chunks} chunks"
        else:
            ready = False
            status = "no_data"
            message = "No documents found. Please upload and embed documents first."
        
        print(f"‚úÖ Stats: {total_documents} docs, {total_chunks} chunks, status={status}")
        
        return {
            "ready": ready,
            "total_documents": total_documents,
            "total_chunks": total_chunks,
            "status": status,
            "message": message,
            "collection_name": "documents",  # Current collection name
            "embedding_model": "text-embedding-004",
            "chat_model": "gemini-2.0-flash-exp"
        }
        
    except Exception as e:
        print(f"‚ùå Error getting stats: {str(e)}")
        
        return {
            "ready": False,
            "total_documents": 0,
            "total_chunks": 0,
            "status": "error",
            "message": f"Error: {str(e)}"
        }
