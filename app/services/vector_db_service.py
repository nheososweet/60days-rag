"""
Vector Database Service using ChromaDB
D·ªãch v·ª• Vector Database s·ª≠ d·ª•ng ChromaDB ƒë·ªÉ l∆∞u tr·ªØ v√† t√¨m ki·∫øm embeddings

=============================================================================
                    LEARNING NOTES - H·ªåC V·ªÄ VECTOR DATABASE
=============================================================================

1. VECTOR DATABASE L√Ä G√å? (What is a Vector Database?)
   =====================================================
   
   ƒê·ªãnh nghƒ©a (Definition):
   - Vector Database = Database chuy√™n bi·ªát ƒë·ªÉ l∆∞u v√† t√¨m ki·∫øm vectors (embeddings)
   - Kh√°c v·ªõi SQL Database: SQL search theo exact match (t√¨m ch√≠nh x√°c)
   - Vector DB: Search theo "similarity" (ƒë·ªô t∆∞∆°ng ƒë·ªìng/gi·ªëng nhau)
   
   T·∫°i sao c·∫ßn? (Why needed?):
   - SQL: "SELECT * WHERE name = 'Python'" ‚Üí ch·ªâ t√¨m ch·ªØ "Python"
   - Vector DB: Query "h·ªçc Python" ‚Üí t√¨m "Python tutorial", "l·∫≠p tr√¨nh Python", "Python course"
   - Hi·ªÉu nghƒ©a thay v√¨ ch·ªâ match ch·ªØ!
   
   Use cases (·ª®ng d·ª•ng):
   - RAG (Retrieval-Augmented Generation): T√¨m context cho AI
   - Semantic Search: T√¨m ki·∫øm theo nghƒ©a
   - Recommendation Systems: "Users th√≠ch A c≈©ng th√≠ch B"
   - Duplicate Detection: T√¨m n·ªôi dung tr√πng l·∫∑p
   - Image/Video Search: T√¨m ·∫£nh t∆∞∆°ng t·ª±

2. CHROMADB - VECTOR DATABASE LIBRARY
   ====================================
   
   ƒê·∫∑c ƒëi·ªÉm (Features):
   - Open-source: Mi·ªÖn ph√≠, code m·ªü
   - Runs locally: Ch·∫°y tr√™n m√°y b·∫°n, kh√¥ng c·∫ßn server ri√™ng
   - Easy to use: API ƒë∆°n gi·∫£n, d·ªÖ h·ªçc
   - Perfect for prototyping: Tuy·ªát v·ªùi ƒë·ªÉ h·ªçc v√† th·ª≠ nghi·ªám
   - Production-ready: C√≥ th·ªÉ scale l√™n production sau
   
   So s√°nh v·ªõi alternatives:
   - Pinecone: Cloud-based, ph·∫£i tr·∫£ ti·ªÅn
   - Weaviate: Ph·ª©c t·∫°p h∆°n, c·∫ßn setup server
   - Milvus: Enterprise-grade, ph·ª©c t·∫°p
   - ChromaDB: ƒê∆°n gi·∫£n nh·∫•t ƒë·ªÉ b·∫Øt ƒë·∫ßu! ‚úÖ
   
   C√†i ƒë·∫∑t (Installation):
   ```bash
   pip install chromadb
   ```

3. SIMILARITY SEARCH - T√åM KI·∫æM THEO ƒê·ªò T∆Ø∆†NG ƒê·ªíNG
   ================================================
   
   C√°ch ho·∫°t ƒë·ªông (How it works):
   
   B∆∞·ªõc 1: User h·ªèi c√¢u h·ªèi
   - Query: "t√¥i mu·ªën h·ªçc Python"
   
   B∆∞·ªõc 2: Embed query th√†nh vector
   - "t√¥i mu·ªën h·ªçc Python" ‚Üí [0.2, 0.5, -0.3, ..., 0.8] (768 s·ªë)
   
   B∆∞·ªõc 3: ChromaDB so s√°nh v·ªõi ALL stored vectors
   - Vector DB c√≥ s·∫µn 1000 chunks
   - Compare query vector v·ªõi 1000 vectors
   - T√≠nh cosine similarity (ƒë·ªô gi·ªëng nhau)
   
   B∆∞·ªõc 4: Tr·∫£ v·ªÅ top-k most similar chunks
   - Chunk 1: "Python tutorial for beginners" (similarity: 0.92)
   - Chunk 2: "How to learn Python programming" (similarity: 0.89)
   - Chunk 3: "Python course online" (similarity: 0.85)
   
   Cosine Similarity - ƒê·ªô ƒëo t∆∞∆°ng ƒë·ªìng:
   -------------------------------------
   - Formula: similarity = cos(angle between vectors)
   - Range: -1 ƒë·∫øn 1
   - 1 = ho√†n to√†n gi·ªëng nhau (identical meaning)
   - 0.8-0.9 = r·∫•t gi·ªëng (very similar)
   - 0.5-0.7 = c√≥ li√™n quan (related)
   - 0 = kh√¥ng li√™n quan (unrelated)
   - -1 = tr√°i ng∆∞·ª£c (opposite meaning)
   
   T·∫°i sao kh√¥ng d√πng exact text match?
   -------------------------------------
   - Exact match: "h·ªçc Python" ch·ªâ match "h·ªçc Python"
   - B·ªè l·ª°: "l·∫≠p tr√¨nh Python", "Python tutorial", "h·ªçc code"
   - Similarity search: T√¨m theo NGHƒ®A, kh√¥ng ch·ªâ t·ª´ kh√≥a!
   - K·∫øt qu·∫£: T√¨m ƒë∆∞·ª£c nhi·ªÅu t√†i li·ªáu li√™n quan h∆°n

4. HOW VECTOR DB WORKS - C√ÅCH HO·∫†T ƒê·ªòNG
   ======================================
   
   A. STORING DATA (L∆∞u tr·ªØ d·ªØ li·ªáu):
   -----------------------------------
   Input: Document chunks + embeddings
   
   Step 1: Prepare data
   - Text chunk: "Python is a programming language"
   - Embedding: [0.1, 0.5, -0.3, ..., 0.2] (768 floats)
   - Metadata: {"document_id": "doc_123", "chunk_index": 0}
   
   Step 2: Add to ChromaDB
   - collection.add(ids, embeddings, documents, metadatas)
   - ChromaDB t·∫°o index ƒë·ªÉ search nhanh
   - Data saved to disk (persist)
   
   Step 3: Ready for search!
   - Vectors indexed b·∫±ng HNSW algorithm
   - Search speed: O(log n) thay v√¨ O(n)
   
   B. SEARCHING DATA (T√¨m ki·∫øm):
   ------------------------------
   Input: User question
   
   Step 1: Embed question
   - Question: "C√°ch c√†i ƒë·∫∑t Python?"
   - Embedding: [0.15, 0.48, -0.25, ..., 0.18]
   
   Step 2: Find similar vectors
   - ChromaDB.query(question_embedding)
   - Compare v·ªõi all stored vectors
   - Use cosine similarity
   
   Step 3: Return top matches
   - Top 5 most similar chunks
   - Include text + metadata + similarity score
   
   Step 4: Use in RAG
   - Send chunks to AI as context
   - AI generates answer based on context

5. COMPLETE WORKFLOW EXAMPLE
   ==========================
   
   Scenario: B·∫°n c√≥ 3 PDFs v·ªÅ Python programming
   
   Step 1: Upload & Process
   ```
   PDF 1: "Python Basics" ‚Üí 10 chunks ‚Üí 10 embeddings
   PDF 2: "Advanced Python" ‚Üí 15 chunks ‚Üí 15 embeddings  
   PDF 3: "Python Projects" ‚Üí 8 chunks ‚Üí 8 embeddings
   Total: 33 chunks trong ChromaDB
   ```
   
   Step 2: User h·ªèi
   ```
   Question: "L√†m sao ƒë·ªÉ h·ªçc Python cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu?"
   ```
   
   Step 3: Search
   ```
   - Embed question ‚Üí vector
   - Search trong 33 chunks
   - Find top 5 similar:
     1. "Python basics for beginners" (from PDF 1)
     2. "Getting started with Python" (from PDF 1)
     3. "Python tutorial introduction" (from PDF 1)
     4. "How to start learning Python" (from PDF 2)
     5. "Python beginner projects" (from PDF 3)
   ```
   
   Step 4: Build Context
   ```
   Context = Combine 5 chunks into one text
   Send to AI: "Based on this context, answer: [question]"
   AI generates answer using the retrieved information
   ```
   
   K·∫øt qu·∫£ (Result):
   - User gets accurate answer t·ª´ documents
   - AI kh√¥ng "hallucinate" (b·ªãa ƒë·∫∑t)
   - C√≥ source citations (tr√≠ch d·∫´n ngu·ªìn)

=============================================================================
"""

import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any, Optional
import os
import uuid


class VectorDBService:
    """
    Service for managing vector storage and retrieval with ChromaDB
    """
    
    def __init__(self, persist_directory: str = "./chroma_db"):
        """
        Kh·ªüi t·∫°o ChromaDB client (Initialize ChromaDB client)
        
        =============================================================================
        LEARNING - CHROMADB SETUP (THI·∫æT L·∫¨P CHROMADB)
        =============================================================================
        
        persist_directory - Th∆∞ m·ª•c l∆∞u tr·ªØ d·ªØ li·ªáu:
        --------------------------------------------
        - Path: Where ChromaDB saves data on disk (N∆°i ChromaDB l∆∞u data l√™n ƒëƒ©a)
        - Default: "./chroma_db" (th∆∞ m·ª•c trong project)
        
        T·∫°i sao c·∫ßn persist_directory?
        - N·∫øu KH√îNG specify: Data ch·ªâ l∆∞u trong RAM
        - Khi restart program ‚Üí Data m·∫•t h·∫øt! ‚ùå
        - With persist_directory: Data save to disk permanently ‚úÖ
        - Restart program ‚Üí Data v·∫´n c√≤n!
        
        Collections - Gi·ªëng nh∆∞ "tables" trong SQL:
        ------------------------------------------
        - M·ªói collection = m·ªôt nh√≥m documents
        - Example:
          * Collection "products": L∆∞u product descriptions
          * Collection "articles": L∆∞u blog articles
          * Collection "documents": L∆∞u uploaded PDFs/DOCXs
        
        - C√≥ th·ªÉ c√≥ nhi·ªÅu collections trong 1 database
        - M·ªói collection c√≥ settings ri√™ng (distance metric, etc.)
        
        PersistentClient vs Client:
        --------------------------
        - PersistentClient: L∆∞u data to disk (d√πng cho production)
        - Client: Ch·ªâ l∆∞u trong memory (d√πng cho testing)
        - Ch√∫ng ta d√πng PersistentClient ƒë·ªÉ data kh√¥ng m·∫•t!
        
        Args:
            persist_directory: Path ƒë·∫øn th∆∞ m·ª•c l∆∞u ChromaDB data
                              M·∫∑c ƒë·ªãnh: "./chroma_db"
        
        Creates:
            - Th∆∞ m·ª•c chroma_db/ n·∫øu ch∆∞a t·ªìn t·∫°i
            - File chroma.sqlite3 (metadata database)
            - UUID folders ch·ª©a vector data
        """
        self.persist_directory = persist_directory
        
        # Create directory if not exists
        os.makedirs(persist_directory, exist_ok=True)
        
        # Initialize ChromaDB client
        # LEARNING: PersistentClient saves data to disk
        self.client = chromadb.PersistentClient(
            path=persist_directory
        )
        
        # Default collection name
        self.collection_name = "documents"
        
        print(f"‚úÖ Vector DB initialized at: {persist_directory}")
    
    
    def get_or_create_collection(self, name: str = None) -> chromadb.Collection:
        """
        L·∫•y collection ƒë√£ c√≥ ho·∫∑c t·∫°o m·ªõi (Get existing collection or create new one)
        
        =============================================================================
        LEARNING - COLLECTIONS (B·ªò S∆ØU T·∫¨P)
        =============================================================================
        
        Collection l√† g√¨? (What is a Collection?)
        -----------------------------------------
        - Collection = container (th√πng ch·ª©a) for embeddings
        - Gi·ªëng nh∆∞ "table" trong SQL database
        - M·ªói collection c√≥ name ri√™ng: "documents", "products", etc.
        - M·ªói collection c√≥ settings ri√™ng (distance metric, index type)
        
        Metadata c·ªßa Collection:
        -----------------------
        Khi t·∫°o collection, ch√∫ng ta config:
        
        1. Distance Metric (C√°ch ƒëo kho·∫£ng c√°ch):
           - "cosine": Most common cho text (ƒëo g√≥c gi·ªØa vectors)
           - "l2": Euclidean distance (kho·∫£ng c√°ch th·∫≥ng)
           - "ip": Inner product (t√≠ch v√¥ h∆∞·ªõng)
        
        2. HNSW (Hierarchical Navigable Small World):
           - Algorithm ƒë·ªÉ search nhanh
           - Thay v√¨ check ALL vectors ‚Üí ch·ªâ check m·ªôt ph·∫ßn
           - Speed: O(log n) instead of O(n)
           - Trade-off: Speed vs Accuracy
        
        Distance Metrics Chi Ti·∫øt:
        --------------------------
        
        A. COSINE SIMILARITY (Ch√∫ng ta d√πng c√°i n√†y):
           - Measures: G√≥c gi·ªØa 2 vectors
           - Range: -1 to 1
           - Use case: Text similarity (nghƒ©a gi·ªëng nhau)
           - Example:
             * "Python programming" vs "Python coding" ‚Üí 0.95 (g·∫ßn)
             * "Python" vs "Java" ‚Üí 0.5 (c√≥ li√™n quan)
             * "Python" vs "banana" ‚Üí 0.1 (kh√¥ng li√™n quan)
           
           - T·∫°i sao t·ªët cho text?
             * Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi length (ƒë·ªô d√†i)
             * "Python" v√† "Python programming language" ‚Üí v·∫´n similar
             * Focus on direction (h∆∞·ªõng), not magnitude (ƒë·ªô l·ªõn)
        
        B. L2 (EUCLIDEAN DISTANCE):
           - Measures: Kho·∫£ng c√°ch th·∫≥ng gi·ªØa 2 ƒëi·ªÉm
           - Range: 0 to ‚àû
           - Use case: Image embeddings, spatial data
           - Problem v·ªõi text: B·ªã ·∫£nh h∆∞·ªüng b·ªüi vector length
        
        C. IP (INNER PRODUCT):
           - Measures: Dot product c·ªßa 2 vectors
           - Range: -‚àû to ‚àû
           - Use case: Recommendation systems
           - Fast nh∆∞ng less intuitive cho text
        
        Try-Except Pattern:
        ------------------
        ```python
        try:
            collection = self.client.get_collection(name)  # Th·ª≠ l·∫•y
            # N·∫øu collection ƒë√£ t·ªìn t·∫°i ‚Üí success!
        except:
            collection = self.client.create_collection(name)  # Kh√¥ng c√≥ ‚Üí t·∫°o m·ªõi
        ```
        
        - T·∫°i sao d√πng pattern n√†y?
          * Avoid duplicate collections (tr√°nh t·∫°o tr√πng)
          * Safe: Kh√¥ng crash n·∫øu collection ƒë√£ t·ªìn t·∫°i
          * Idempotent: G·ªçi nhi·ªÅu l·∫ßn c≈©ng OK
        
        Args:
            name: T√™n collection (default: "documents")
                  String identifier, unique trong database
            
        Returns:
            chromadb.Collection object ƒë·ªÉ th·ª±c hi·ªán operations:
            - collection.add(): Th√™m embeddings
            - collection.query(): T√¨m ki·∫øm
            - collection.get(): L·∫•y data
            - collection.delete(): X√≥a
            - collection.count(): ƒê·∫øm s·ªë l∆∞·ª£ng
        """
        if name is None:
            name = self.collection_name
        
        try:
            # Try to get existing collection
            collection = self.client.get_collection(name=name)
            print(f"üìö Retrieved existing collection: {name}")
            
        except:
            # Create new collection if doesn't exist
            # LEARNING: metadata configures behavior
            collection = self.client.create_collection(
                name=name,
                metadata={
                    "hnsw:space": "cosine"  # Use cosine similarity
                }
            )
            print(f"üìö Created new collection: {name}")
        
        return collection
    
    
    def add_document(
        self,
        document_id: str,
        embeddings_data: List[Dict[str, Any]],
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Th√™m document embeddings v√†o vector database (Add document embeddings to vector database)
        
        =============================================================================
        LEARNING - STORING EMBEDDINGS (L∆ØU TR·ªÆ EMBEDDINGS)
        =============================================================================
        
        ChromaDB l∆∞u 4 th·ª© cho m·ªói entry:
        ---------------------------------
        1. ID (ƒê·ªãnh danh duy nh·∫•t):
           - Format: "doc_123::chunk_0", "doc_123::chunk_1"
           - M·ªói chunk c√≥ ID ri√™ng
           - D√πng ƒë·ªÉ retrieve/delete specific chunks sau n√†y
        
        2. Embedding (Vector 768 chi·ªÅu):
           - Array of 768 floats: [0.123, -0.456, ..., 0.789]
           - ƒê√¢y l√† "b·∫£n ch·∫•t" c·ªßa text d∆∞·ªõi d·∫°ng s·ªë
           - ChromaDB d√πng vector n√†y ƒë·ªÉ t√≠nh similarity
        
        3. Document (Text th·ª±c t·∫ø):
           - Original text c·ªßa chunk
           - V√≠ d·ª•: "Python is a programming language..."
           - ƒê·ªÉ return cho user khi search
        
        4. Metadata (Th√¥ng tin b·ªï sung):
           - document_id: ID c·ªßa document g·ªëc
           - chunk_index: V·ªã tr√≠ chunk (0, 1, 2, ...)
           - filename: T√™n file g·ªëc
           - words: S·ªë t·ª´ trong chunk
           - length: S·ªë k√Ω t·ª±
           - Custom fields: B·∫•t k·ª≥ info n√†o b·∫°n mu·ªën
        
        T·∫°i sao c·∫ßn ID ri√™ng cho m·ªói chunk?
        ------------------------------------
        Problem: 1 document ‚Üí nhi·ªÅu chunks
        - Document: "report.pdf" c√≥ 50 pages
        - After chunking: 100 chunks
        
        Solution: Unique ID format
        - Chunk 1: "doc_123::chunk_0"
        - Chunk 2: "doc_123::chunk_1"
        - ...
        - Chunk 100: "doc_123::chunk_99"
        
        Benefits:
        - Can delete all chunks of 1 document: Filter by "doc_123"
        - Can get specific chunk: Get by "doc_123::chunk_5"
        - Track source: Know which document a chunk came from
        - Order preserved: chunk_index maintains sequence
        
        Data Structure Example:
        ----------------------
        embeddings_data = [
            {
                "chunk_index": 0,
                "text": "Python is a programming language...",
                "embedding": [0.1, 0.5, -0.3, ..., 0.2],  # 768 floats
                "metadata": {
                    "length": 3000,
                    "words": 500
                }
            },
            {
                "chunk_index": 1,
                "text": "Python supports multiple paradigms...",
                "embedding": [0.15, 0.48, -0.25, ..., 0.18],
                "metadata": {
                    "length": 2950,
                    "words": 495
                }
            }
        ]
        
        Metadata Merging:
        ----------------
        - Chunk-level metadata: length, words, chunk_index
        - Document-level metadata: filename, upload_time, file_path
        - Combined: chunk_metadata.update(document_metadata)
        - Result: M·ªói chunk c√≥ BOTH types of metadata
        
        Why?
        - Search by document: where={"filename": "report.pdf"}
        - Filter by chunk size: where={"words": {"$gte": 400}}
        - Flexible querying!
        
        Args:
            document_id: Unique ID cho document (UUID string)
                        V√≠ d·ª•: "550e8400-e29b-41d4-a716-446655440000"
            
            embeddings_data: List of dicts t·ª´ EmbeddingService.process_document()
                            M·ªói dict ch·ª©a: text, embedding, chunk_index, metadata
            
            metadata: Optional document-level metadata (dict)
                     V√≠ d·ª•: {"filename": "report.pdf", "upload_time": "2024-01-01"}
                     S·∫Ω ƒë∆∞·ª£c add v√†o ALL chunks c·ªßa document
            
        Returns:
            Dict v·ªõi stats v·ªÅ chunks ƒë√£ l∆∞u:
            {
                "document_id": "doc_123",
                "chunks_stored": 100,
                "collection": "documents"
            }
        
        Process Flow:
        ------------
        1. Prepare 4 lists: ids, embeddings, documents, metadatas
        2. Loop through embeddings_data
        3. For each chunk:
           - Generate unique ID
           - Extract embedding vector
           - Extract text
           - Merge metadata
           - Append to lists
        4. Call collection.add() with all 4 lists
        5. ChromaDB indexes v√† stores data
        6. Return success stats
        """
        collection = self.get_or_create_collection()
        
        # Prepare data for ChromaDB
        ids = []
        embeddings = []
        documents = []
        metadatas = []
        
        print(f"\nüíæ Storing {len(embeddings_data)} embeddings for document: {document_id}")
        
        for item in embeddings_data:
            # Create unique ID for each chunk
            # LEARNING: Format = "doc_id::chunk_0", "doc_id::chunk_1", etc.
            chunk_id = f"{document_id}::chunk_{item['chunk_index']}"
            ids.append(chunk_id)
            
            # Extract embedding vector
            embeddings.append(item['embedding'])
            
            # Extract text
            documents.append(item['text'])
            
            # Combine metadata
            chunk_metadata = {
                "document_id": document_id,
                "chunk_index": item['chunk_index'],
                "length": item['metadata']['length'],
                "words": item['metadata']['words'],
            }
            
            # Add document-level metadata if provided
            if metadata:
                chunk_metadata.update(metadata)
            
            metadatas.append(chunk_metadata)
        
        # Add to ChromaDB
        # LEARNING: add() is the main storage method
        collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )
        
        print(f"‚úÖ Successfully stored {len(ids)} chunks in vector DB")
        
        return {
            "document_id": document_id,
            "chunks_stored": len(ids),
            "collection": self.collection_name
        }
    
    
    def search(
        self,
        query_embedding: List[float],
        n_results: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        T√¨m ki·∫øm documents t∆∞∆°ng t·ª± b·∫±ng vector similarity (Search for similar documents)
        
        =============================================================================
        LEARNING - SIMILARITY SEARCH (T√åM KI·∫æM THEO ƒê·ªò T∆Ø∆†NG ƒê·ªíNG)
        =============================================================================
        
        C√°ch ho·∫°t ƒë·ªông chi ti·∫øt (Detailed workflow):
        --------------------------------------------
        
        Step 1: User h·ªèi c√¢u h·ªèi
        - Question: "C√°ch h·ªçc Python hi·ªáu qu·∫£?"
        - ƒê√¢y l√† natural language, m√°y t√≠nh ch∆∞a hi·ªÉu ƒë∆∞·ª£c
        
        Step 2: Embed c√¢u h·ªèi th√†nh vector
        - Call embedding_service.embed_text(question)
        - Result: [0.234, -0.567, 0.123, ..., 0.890] (768 floats)
        - Gi·ªù m√°y t√≠nh c√≥ th·ªÉ work v·ªõi n√≥!
        
        Step 3: ChromaDB so s√°nh v·ªõi ALL stored vectors
        - Database c√≥ 1000 chunks (1000 vectors)
        - ChromaDB compare query vector v·ªõi 1000 vectors
        - T√≠nh cosine similarity cho m·ªói pair
        - Rank theo similarity scores
        
        Step 4: Tr·∫£ v·ªÅ top-k most similar chunks
        - n_results = 5 ‚Üí return 5 chunks gi·ªëng nh·∫•t
        - Include: text + metadata + similarity score
        - Sort by score (highest first)
        
        Cosine Similarity - C√¥ng th·ª©c ƒëo ƒë·ªô t∆∞∆°ng ƒë·ªìng:
        -----------------------------------------------
        
        Formula:
        ```
        similarity = (A ¬∑ B) / (||A|| √ó ||B||)
        
        Where:
        - A ¬∑ B = dot product (t√≠ch v√¥ h∆∞·ªõng)
        - ||A|| = magnitude of A (ƒë·ªô d√†i vector A)
        - ||B|| = magnitude of B
        ```
        
        √ù nghƒ©a:
        - Measures: G√≥c gi·ªØa 2 vectors trong kh√¥ng gian 768 chi·ªÅu
        - Kh√¥ng quan t√¢m ƒë·ªô d√†i vector, ch·ªâ quan t√¢m h∆∞·ªõng
        - Vectors c√πng h∆∞·ªõng = meanings gi·ªëng nhau
        
        Range v√† √Ω nghƒ©a:
        - 1.0 = Ho√†n to√†n gi·ªëng nhau (identical meaning)
        - 0.9-0.99 = R·∫•t r·∫•t gi·ªëng (nearly identical)
        - 0.8-0.89 = R·∫•t gi·ªëng (very similar)
        - 0.7-0.79 = Gi·ªëng (similar)
        - 0.5-0.69 = C√≥ li√™n quan (related)
        - 0.3-0.49 = H∆°i li√™n quan (loosely related)
        - 0.0-0.29 = √çt li√™n quan (barely related)
        - 0 = Kh√¥ng li√™n quan (unrelated)
        - -1 = Tr√°i ng∆∞·ª£c (opposite meaning - rare for text)
        
        Example scores th·ª±c t·∫ø:
        ----------------------
        Query: "C√°ch h·ªçc Python hi·ªáu qu·∫£?"
        
        Results:
        1. "How to learn Python effectively" ‚Üí 0.95 ‚úÖ (Perfect match!)
        2. "Python learning tips for beginners" ‚Üí 0.88 ‚úÖ (Great match)
        3. "Best ways to study programming" ‚Üí 0.72 ‚úÖ (Good match)
        4. "Python tutorial for advanced users" ‚Üí 0.65 ‚ö†Ô∏è (OK match)
        5. "Java programming basics" ‚Üí 0.35 ‚ùå (Weak match)
        
        T·∫°i sao kh√¥ng d√πng exact text match?
        ------------------------------------
        
        Problem v·ªõi keyword search:
        - Query: "h·ªçc Python"
        - Keyword search ch·ªâ match documents c√≥ ch·ªØ "h·ªçc" V√Ä "Python"
        - B·ªè l·ª°:
          * "l·∫≠p tr√¨nh Python" (kh√¥ng c√≥ ch·ªØ "h·ªçc")
          * "Python tutorial" (ti·∫øng Anh, kh√¥ng c√≥ "h·ªçc")
          * "Python course online" (t·ª´ ƒë·ªìng nghƒ©a)
          * "getting started with Python" (√Ω nghƒ©a gi·ªëng nh∆∞ng kh√°c t·ª´)
        
        Solution v·ªõi embeddings:
        - Query: "h·ªçc Python" ‚Üí embedding
        - Matches:
          * "l·∫≠p tr√¨nh Python" ‚úÖ (similar meaning)
          * "Python tutorial" ‚úÖ (same concept)
          * "Python course" ‚úÖ (synonymous)
          * "learn programming" ‚úÖ (related concept)
        - Embeddings capture SEMANTIC MEANING, not just keywords!
        
        Metadata Filtering:
        ------------------
        Optional: Filter results by metadata
        
        Example 1: Search trong 1 document specific
        ```python
        results = vector_db.search(
            query_embedding=embedding,
            n_results=5,
            filter_metadata={"document_id": "doc_123"}
        )
        # Ch·ªâ search trong chunks c·ªßa doc_123
        ```
        
        Example 2: Filter by filename
        ```python
        filter_metadata={"filename": "python_guide.pdf"}
        # Ch·ªâ search trong chunks t·ª´ file n√†y
        ```
        
        Example 3: Filter by chunk size
        ```python
        filter_metadata={"words": {"$gte": 400}}
        # Ch·ªâ l·∫•y chunks c√≥ >= 400 words
        ```
        
        Args:
            query_embedding: 768d vector c·ªßa user's question
                           List of 768 floats
                           V√≠ d·ª•: [0.234, -0.567, ..., 0.890]
            
            n_results: S·ªë l∆∞·ª£ng similar chunks mu·ªën return
                      Default: 5 (top 5 matches)
                      Increase n·∫øu c·∫ßn more context
            
            filter_metadata: Optional dict ƒë·ªÉ filter results
                           V√≠ d·ª•: {"document_id": "doc_123"}
                           None = search ALL chunks
            
        Returns:
            Dict v·ªõi structure:
            {
                "results": [
                    {
                        "id": "doc_123::chunk_0",
                        "text": "Full chunk text...",
                        "metadata": {...},
                        "distance": 0.15  # Lower = more similar
                    },
                    ...
                ],
                "count": 5
            }
            
        Note: distance vs similarity
        - ChromaDB returns "distance" (kho·∫£ng c√°ch)
        - Lower distance = more similar
        - Distance ‚âà 0.0 = very similar
        - Distance > 1.0 = not similar
        - Similarity = 1 - distance (approximately)
        """
        collection = self.get_or_create_collection()
        
        print(f"\nüîç Searching for {n_results} similar chunks...")
        
        # Search in ChromaDB
        # LEARNING: query() finds most similar vectors
        results = collection.query(
            query_embeddings=[query_embedding],  # Must be list of lists
            n_results=n_results,
            where=filter_metadata  # Optional filter
        )
        
        # Parse results
        # LEARNING: ChromaDB returns lists for batch queries
        # We only query 1 embedding, so take [0] index
        matched_docs = []
        
        if results['ids'] and len(results['ids'][0]) > 0:
            for i in range(len(results['ids'][0])):
                matched_docs.append({
                    "id": results['ids'][0][i],
                    "text": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "distance": results['distances'][0][i] if 'distances' in results else None
                })
        
        print(f"‚úÖ Found {len(matched_docs)} matching chunks")
        
        # Log top result for debugging
        if matched_docs:
            top_match = matched_docs[0]
            print(f"   Top match (distance: {top_match['distance']:.4f}):")
            print(f"   {top_match['text'][:100]}...")
        
        return {
            "results": matched_docs,
            "count": len(matched_docs)
        }
    
    
    def delete_document(self, document_id: str) -> Dict[str, Any]:
        """
        X√≥a t·∫•t c·∫£ chunks c·ªßa m·ªôt document (Delete all chunks of a document)
        
        =============================================================================
        LEARNING - DELETION (X√ìA D·ªÆ LI·ªÜU)
        =============================================================================
        
        Use case - Khi n√†o c·∫ßn x√≥a?
        ---------------------------
        1. User deletes uploaded PDF t·ª´ UI
        2. Document outdated, c·∫ßn upload version m·ªõi
        3. Cleanup: Remove old/unused documents
        4. Privacy: User requests data deletion
        
        Problem: 1 document = nhi·ªÅu chunks
        ----------------------------------
        - Document "report.pdf" ‚Üí 50 chunks
        - Chunk IDs: "doc_123::chunk_0" ƒë·∫øn "doc_123::chunk_49"
        - C·∫ßn x√≥a ALL 50 chunks, kh√¥ng ph·∫£i ch·ªâ 1!
        
        Solution: Metadata filtering
        ----------------------------
        Step 1: Find all chunks c·ªßa document
        - Query ChromaDB v·ªõi filter: {"document_id": "doc_123"}
        - ChromaDB returns list of matching chunk IDs
        
        Step 2: Delete by IDs
        - collection.delete(ids=[list_of_ids])
        - All chunks removed in one operation
        
        Step 3: Verify v√† return stats
        - Count s·ªë chunks deleted
        - Return success status
        
        Metadata Query Syntax:
        ---------------------
        ChromaDB supports MongoDB-style queries:
        
        Exact match:
        ```python
        where={"document_id": "doc_123"}
        # Find chunks where document_id == "doc_123"
        ```
        
        Multiple conditions:
        ```python
        where={
            "document_id": "doc_123",
            "chunk_index": {"$gte": 10}
        }
        # Find chunks where document_id == "doc_123" AND chunk_index >= 10
        ```
        
        OR conditions:
        ```python
        where={
            "$or": [
                {"document_id": "doc_123"},
                {"document_id": "doc_456"}
            ]
        }
        # Find chunks from either document
        ```
        
        Safety Considerations:
        ---------------------
        1. Deletion is PERMANENT!
           - Cannot undo after delete
           - Data gone from ChromaDB
           - Consider "soft delete" for production
        
        2. Cascading deletes:
           - Delete document from vector DB
           - Should also delete file from disk?
           - Currently: File kept, only embeddings deleted
           - Can re-embed if needed
        
        3. Error handling:
           - Document not found ‚Üí return error
           - Partial delete failures ‚Üí rollback?
           - Log deletion events for audit trail
        
        Alternative: Soft Delete
        -----------------------
        Instead of deleting, mark as "deleted":
        ```python
        # Update metadata
        collection.update(
            ids=chunk_ids,
            metadatas=[{"deleted": True, ...}]
        )
        
        # Search excludes deleted
        where={"deleted": {"$ne": True}}
        ```
        
        Benefits:
        - Can recover if mistake
        - Audit trail preserved
        - Gradual cleanup possible
        
        Args:
            document_id: ID c·ªßa document c·∫ßn x√≥a (string UUID)
                        V√≠ d·ª•: "550e8400-e29b-41d4-a716-446655440000"
            
        Returns:
            Dict v·ªõi deletion status:
            
            Success case:
            {
                "success": True,
                "document_id": "doc_123",
                "chunks_deleted": 50
            }
            
            Not found case:
            {
                "success": False,
                "message": "Document not found"
            }
            
            Error case:
            {
                "success": False,
                "error": "Error message here"
            }
        
        Process Flow:
        ------------
        1. Get collection
        2. Query chunks v·ªõi where={"document_id": ...}
        3. Check if any chunks found
        4. If yes:
           - Delete by IDs
           - Count deleted
           - Return success
        5. If no:
           - Return not found error
        6. If exception:
           - Catch error
           - Return error message
        """
        collection = self.get_or_create_collection()
        
        print(f"\nüóëÔ∏è  Deleting document: {document_id}")
        
        try:
            # Find all chunks for this document
            # LEARNING: Use where filter to find by metadata
            results = collection.get(
                where={"document_id": document_id}
            )
            
            if results['ids']:
                # Delete by IDs
                collection.delete(ids=results['ids'])
                print(f"‚úÖ Deleted {len(results['ids'])} chunks")
                
                return {
                    "success": True,
                    "document_id": document_id,
                    "chunks_deleted": len(results['ids'])
                }
            else:
                print(f"‚ö†Ô∏è  No chunks found for document: {document_id}")
                return {
                    "success": False,
                    "message": "Document not found"
                }
                
        except Exception as e:
            print(f"‚ùå Delete error: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """
        Get statistics about stored documents
        
        Returns:
            Dict with collection stats
        """
        collection = self.get_or_create_collection()
        count = collection.count()
        
        return {
            "collection_name": self.collection_name,
            "total_chunks": count,
            "persist_directory": self.persist_directory
        }
    
    
    def list_all_documents(self) -> List[Dict[str, Any]]:
        """
        L·∫•y danh s√°ch T·∫§T C·∫¢ documents ƒë√£ embedding (List all embedded documents)
        
        =============================================================================
        LEARNING - QU·∫¢N L√ù DOCUMENTS TRONG VECTOR DB
        =============================================================================
        
        C√°ch l∆∞u tr·ªØ (Storage structure):
        ---------------------------------
        M·ªói document ƒë∆∞·ª£c chia th√†nh nhi·ªÅu chunks:
        - doc_123::chunk_0
        - doc_123::chunk_1
        - doc_123::chunk_2
        
        Method n√†y:
        1. L·∫•y ALL chunks t·ª´ ChromaDB
        2. Group theo document_id
        3. Aggregate metadata (filename, upload time, etc.)
        4. Count s·ªë chunks per document
        
        Use case: Hi·ªÉn th·ªã list documents trong UI
        
        Returns:
            List of dicts, m·ªói dict = 1 document v·ªõi:
            - document_id: Unique ID
            - filename: T√™n file g·ªëc
            - chunks_count: S·ªë chunks
            - metadata: Th√¥ng tin kh√°c (upload time, size, etc.)
        """
        collection = self.get_or_create_collection()
        
        try:
            # Get ALL data from collection
            # LEARNING: get() without filters returns everything
            all_data = collection.get(
                include=["metadatas", "documents"]  # Include metadata v√† text
            )
            
            if not all_data['ids'] or len(all_data['ids']) == 0:
                print("üìö No documents found in vector DB")
                return []
            
            # Group chunks by document_id
            # LEARNING: D√πng dict ƒë·ªÉ group, key = document_id
            documents_map = {}
            
            for i, chunk_id in enumerate(all_data['ids']):
                metadata = all_data['metadatas'][i]
                document_id = metadata.get('document_id')
                
                if not document_id:
                    continue
                
                # Initialize document entry n·∫øu ch∆∞a c√≥
                if document_id not in documents_map:
                    documents_map[document_id] = {
                        "document_id": document_id,
                        "filename": metadata.get('filename', 'Unknown'),
                        "chunks_count": 0,
                        "total_words": 0,
                        "metadata": {}
                    }
                    
                    # Copy metadata (ch·ªâ l∆∞u 1 l·∫ßn t·ª´ chunk ƒë·∫ßu ti√™n)
                    for key, value in metadata.items():
                        if key not in ['document_id', 'chunk_index', 'length', 'words']:
                            documents_map[document_id]["metadata"][key] = value
                
                # Increment counts
                documents_map[document_id]["chunks_count"] += 1
                documents_map[document_id]["total_words"] += metadata.get('words', 0)
            
            # Convert map to list
            documents_list = list(documents_map.values())
            
            print(f"üìö Found {len(documents_list)} documents with {len(all_data['ids'])} total chunks")
            
            return documents_list
            
        except Exception as e:
            print(f"‚ùå Error listing documents: {str(e)}")
            return []
    
    
    def get_document_by_id(self, document_id: str) -> Optional[Dict[str, Any]]:
        """
        L·∫•y chi ti·∫øt 1 document c·ª• th·ªÉ (Get details of a specific document)
        
        =============================================================================
        LEARNING - RETRIEVE DOCUMENT DETAILS
        =============================================================================
        
        Method n√†y tr·∫£ v·ªÅ:
        1. Document metadata (filename, upload time, etc.)
        2. List ALL chunks c·ªßa document
        3. Chunk details (text preview, word count, etc.)
        
        Use case: 
        - User clicks v√†o 1 document trong UI
        - Hi·ªÉn th·ªã chi ti·∫øt + preview chunks
        - Debug: Xem chunks c√≥ ƒë√∫ng kh√¥ng
        
        Args:
            document_id: ID c·ªßa document c·∫ßn l·∫•y
        
        Returns:
            Dict v·ªõi document info + list of chunks, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        collection = self.get_or_create_collection()
        
        try:
            # Query chunks c·ªßa document n√†y
            # LEARNING: Use where filter ƒë·ªÉ query by metadata
            results = collection.get(
                where={"document_id": document_id},
                include=["metadatas", "documents"]
            )
            
            if not results['ids'] or len(results['ids']) == 0:
                print(f"‚ö†Ô∏è  Document not found: {document_id}")
                return None
            
            # Prepare document info
            first_metadata = results['metadatas'][0]
            
            document_info = {
                "document_id": document_id,
                "filename": first_metadata.get('filename', 'Unknown'),
                "chunks_count": len(results['ids']),
                "chunks": []
            }
            
            # Add metadata
            for key, value in first_metadata.items():
                if key not in ['document_id', 'chunk_index', 'length', 'words']:
                    document_info[key] = value
            
            # Add all chunks with details
            for i in range(len(results['ids'])):
                chunk_info = {
                    "chunk_id": results['ids'][i],
                    "chunk_index": results['metadatas'][i].get('chunk_index', i),
                    "text": results['documents'][i],
                    "text_preview": results['documents'][i][:200] + "..." if len(results['documents'][i]) > 200 else results['documents'][i],
                    "words": results['metadatas'][i].get('words', 0),
                    "length": results['metadatas'][i].get('length', 0)
                }
                document_info["chunks"].append(chunk_info)
            
            # Sort chunks by index
            document_info["chunks"].sort(key=lambda x: x['chunk_index'])
            
            print(f"üìÑ Retrieved document: {document_id} with {len(results['ids'])} chunks")
            
            return document_info
            
        except Exception as e:
            print(f"‚ùå Error getting document: {str(e)}")
            return None
    
    
    def get_document_chunks(self, document_id: str) -> List[str]:
        """
        L·∫•y ONLY text content c·ªßa all chunks (Get only text of all chunks)
        
        =============================================================================
        LEARNING - LIGHTWEIGHT RETRIEVAL
        =============================================================================
        
        Kh√°c v·ªõi get_document_by_id():
        - Method n√†y CH·ªà tr·∫£ v·ªÅ text, kh√¥ng c√≥ metadata
        - Lighter weight ‚Üí faster
        - Use case: Khi c·∫ßn full text ƒë·ªÉ process/display
        
        Args:
            document_id: ID c·ªßa document
        
        Returns:
            List of chunk texts (strings), sorted by chunk_index
        """
        collection = self.get_or_create_collection()
        
        try:
            results = collection.get(
                where={"document_id": document_id},
                include=["metadatas", "documents"]
            )
            
            if not results['ids'] or len(results['ids']) == 0:
                return []
            
            # Create list of (chunk_index, text) tuples
            chunks_with_index = []
            for i in range(len(results['ids'])):
                chunk_index = results['metadatas'][i].get('chunk_index', i)
                text = results['documents'][i]
                chunks_with_index.append((chunk_index, text))
            
            # Sort by chunk_index
            chunks_with_index.sort(key=lambda x: x[0])
            
            # Return only texts
            texts = [text for _, text in chunks_with_index]
            
            return texts
            
        except Exception as e:
            print(f"‚ùå Error getting chunks: {str(e)}")
            return []


# LEARNING - COMPLETE RAG FLOW:
# =============================
"""
End-to-end example:

# 1. Setup services
from embedding_service import EmbeddingService
from vector_db_service import VectorDBService

embedding_service = EmbeddingService()
vector_db = VectorDBService()

# 2. Process & store document
document_text = "Your PDF content here..."
document_id = "doc_123"

# Embed
embeddings = embedding_service.process_document(document_text)

# Store
vector_db.add_document(
    document_id=document_id,
    embeddings_data=embeddings,
    metadata={"filename": "report.pdf"}
)

# 3. Search (when user asks question)
question = "Summarize the main points"

# Embed question
question_embedding = embedding_service.embed_text(question)

# Find similar chunks
results = vector_db.search(
    query_embedding=question_embedding,
    n_results=3
)

# 4. Send to Gemini with context
context = "\n\n".join([r['text'] for r in results['results']])
prompt = f"Context:\n{context}\n\nQuestion: {question}\n\nAnswer:"

# ‚Üí Send to Gemini ‚Üí Get response ‚Üí Return to user!
"""
