"""
Embedding Service - D·ªãch v·ª• t·∫°o Embeddings
X·ª≠ l√Ω vi·ªác chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh vectors s·ª≠ d·ª•ng Gemini text-embedding-004

=============================================================================
                    LEARNING NOTES - H·ªåC V·ªÄ EMBEDDINGS
=============================================================================

1. EMBEDDINGS L√Ä G√å? (What are Embeddings?)
   =========================================
   
   - ƒê·ªãnh nghƒ©a ƒë∆°n gi·∫£n (Simple definition):
     * Embeddings = c√°ch bi·ªÉu di·ªÖn text b·∫±ng s·ªë (numerical representation)
     * Chuy·ªÉn ƒë·ªïi t·ª´, c√¢u, ƒëo·∫°n vƒÉn ‚Üí m·∫£ng c√°c s·ªë th·ª±c (array of floats)
     * M·ªói text ‚Üí 1 ƒëi·ªÉm trong kh√¥ng gian nhi·ªÅu chi·ªÅu (multidimensional space)
   
   - V√≠ d·ª• c·ª• th·ªÉ (Concrete example):
     * Input text: "con m√®o"
     * Output vector: [0.1, 0.5, -0.3, ..., 0.2] (768 s·ªë th·ª±c)
     * Input text: "con m√®o cute"  
     * Output vector: [0.12, 0.48, -0.28, ..., 0.21] (r·∫•t g·∫ßn v·ªõi "con m√®o")
     * Input text: "xe h∆°i"
     * Output vector: [0.8, -0.2, 0.5, ..., -0.4] (r·∫•t kh√°c "con m√®o")
   
   - T√≠nh ch·∫•t quan tr·ªçng (Key properties):
     * Semantic similarity: Nghƒ©a gi·ªëng ‚Üí vectors g·∫ßn nhau
     * C√°c text v·ªÅ "ƒë·ªông v·∫≠t" s·∫Ω cluster l·∫°i g·∫ßn nhau
     * C√°c text v·ªÅ "xe c·ªô" s·∫Ω ·ªü v√πng kh√°c
     * Distance between vectors = ƒë·ªô kh√°c bi·ªát v·ªÅ nghƒ©a

2. T·∫†I SAO C·∫¶N EMBEDDINGS? (Why do we need Embeddings?)
   ====================================================
   
   - V·∫•n ƒë·ªÅ c∆° b·∫£n (Core problem):
     * M√°y t√≠nh ch·ªâ hi·ªÉu s·ªë, kh√¥ng hi·ªÉu nghƒ©a c·ªßa ch·ªØ
     * "Python programming" v√† "Python coding" = nghƒ©a g·∫ßn gi·ªëng
     * Nh∆∞ng m√°y t√≠nh th·∫•y 2 strings kh√°c nhau ho√†n to√†n!
   
   - Gi·∫£i ph√°p (Solution):
     * Embeddings gi√∫p m√°y t√≠nh "hi·ªÉu" semantic meaning
     * Hai c√¢u nghƒ©a gi·ªëng ‚Üí embeddings gi·ªëng nhau
     * C√≥ th·ªÉ t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng b·∫±ng math!
   
   - So s√°nh c√°ch t√¨m ki·∫øm (Search comparison):
   
     A. Keyword Search (C√°ch c≈©):
        - Query: "h·ªçc Python"
        - Ch·ªâ match: documents c√≥ ch·ªØ "h·ªçc" V√Ä "Python"
        - B·ªè l·ª°: "l·∫≠p tr√¨nh Python", "Python tutorial", "kh√≥a Python"
        - Problem: Too strict, misses relevant content
     
     B. Semantic Search (D√πng Embeddings):
        - Query: "h·ªçc Python"
        - Embed query ‚Üí [0.2, 0.5, ...]
        - T√¨m documents c√≥ embeddings g·∫ßn v·ªõi query embedding
        - Match: "l·∫≠p tr√¨nh Python" ‚úÖ, "Python tutorial" ‚úÖ, "kh√≥a h·ªçc coding" ‚úÖ
        - Smart: Hi·ªÉu nghƒ©a, kh√¥ng ch·ªâ keywords!
   
   - ·ª®ng d·ª•ng th·ª±c t·∫ø (Real-world applications):
     * RAG (Retrieval-Augmented Generation): T√¨m context cho AI
     * Recommendation: "Users th√≠ch A c≈©ng th√≠ch B"
     * Duplicate detection: T√¨m c√¢u h·ªèi tr√πng l·∫∑p
     * Classification: Ph√¢n lo·∫°i theo ch·ªß ƒë·ªÅ

3. GEMINI TEXT-EMBEDDING-004 MODEL
   =================================
   
   - Th√¥ng s·ªë k·ªπ thu·∫≠t (Technical specs):
     * Model name: "text-embedding-004"
     * Dimensions: 768 (m·ªói embedding = 768 s·ªë th·ª±c)
     * Max input: 2048 tokens (~8000 characters ti·∫øng Anh)
     * Output type: Fixed-size vector [float32 x 768]
     * Pricing: FREE! üéâ (1,500 requests/day limit)
   
   - T·∫°i sao ch·ªçn 768 dimensions?
     * Trade-off: accuracy vs performance
     * 768 dims ƒë·ªß ƒë·ªÉ capture complex semantic relationships
     * Kh√¥ng qu√° l·ªõn ‚Üí fast computation v√† storage
     * Industry standard (BERT, nhi·ªÅu models kh√°c c≈©ng d√πng 768)
   
   - Quality benchmark:
     * SOTA (State-of-the-art) cho semantic search
     * Multilingual: Support ti·∫øng Vi·ªát, English, etc.
     * Trained tr√™n massive text corpus
     * Very good at capturing nuanced meanings

4. CHUNKING STRATEGY (Chi·∫øn l∆∞·ª£c chia nh·ªè vƒÉn b·∫£n)
   ================================================
   
   - T·∫°i sao ph·∫£i chunk? (Why chunk?)
     * Gi·ªõi h·∫°n model: Max 2048 tokens per request
     * Long documents: S√°ch 300 trang = h√†ng tri·ªáu tokens!
     * Cannot embed entire book in one go
   
   - Chunk size: ~500 words (K√≠ch th∆∞·ªõc m·ªói ƒëo·∫°n)
     * L√Ω do ch·ªçn 500 words:
       1. ƒê·ªß context: ƒêo·∫°n vƒÉn c√≥ √Ω nghƒ©a ho√†n ch·ªânh
       2. Not too long: Trong gi·ªõi h·∫°n model
       3. Not too short: Tr√°nh m·∫•t ng·ªØ c·∫£nh
       4. Optimal for search: Balance between precision v√† recall
   
   - Overlap: 50 words (Ph·∫ßn ch·ªìng l·∫•p)
     * V√≠ d·ª•:
       Chunk 1: words 1-500
       Chunk 2: words 451-950 (overlap 50 words with chunk 1)
       Chunk 3: words 901-1400
     
     * T·∫°i sao overlap?
       - Tr√°nh m·∫•t th√¥ng tin ·ªü boundaries (ranh gi·ªõi chunks)
       - C√¢u b·ªã c·∫Øt gi·ªØa 2 chunks v·∫´n xu·∫•t hi·ªán ho√†n ch·ªânh ·ªü 1 chunk
       - Improves recall: TƒÉng kh·∫£ nƒÉng t√¨m th·∫•y relevant info
   
   - Visual example (Minh h·ªça):
     ```
     Original text: "... AI is powerful. Machine learning helps AI. Deep learning is advanced..."
     
     Without overlap:
     Chunk 1: "... AI is powerful."
     Chunk 2: "Machine learning helps AI."  ‚ùå Lost connection!
     
     With overlap:
     Chunk 1: "... AI is powerful. Machine learning helps AI."
     Chunk 2: "Machine learning helps AI. Deep learning is advanced..."  ‚úÖ Context preserved!
     ```

5. COMPLETE WORKFLOW (Quy tr√¨nh ho√†n ch·ªânh)
   =========================================
   
   Step 1: Upload document
   ‚Üí PDF/DOCX file
   
   Step 2: Extract text  
   ‚Üí Raw text string
   
   Step 3: Chunk text (chia nh·ªè)
   ‚Üí List of ~500-word chunks with overlap
   
   Step 4: Embed each chunk (t·∫°o embeddings)
   ‚Üí List of 768-d vectors
   
   Step 5: Store in Vector DB (l∆∞u v√†o ChromaDB)
   ‚Üí Ready for semantic search!
   
   Step 6: Query (khi user h·ªèi)
   ‚Üí Embed question ‚Üí Find similar chunks ‚Üí Send to AI ‚Üí Get answer

=============================================================================
"""

from google import genai
from google.genai import types
import os
from typing import List, Dict, Any
import time
from app.core import settings

class EmbeddingService:
    """
    Service to handle text embeddings using Gemini
    """
    
    def __init__(self):
        """
        Initialize Gemini client for embeddings
        
        LEARNING: Gemini SDK cung c·∫•p ri√™ng embedding API
        Kh√¥ng c·∫ßn g·ªçi generate_content, d√πng embed_content()
        """
        api_key = settings.GEMINI_API_KEY
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")
        
        # Initialize client
        self.client = genai.Client(api_key=api_key)
        
        # Model name cho embedding
        # LEARNING: text-embedding-004 l√† latest v√† best cho semantic search
        self.model = "models/text-embedding-004"
        
        print(f"‚úÖ Embedding service initialized with model: {self.model}")
    
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """
        Chia vƒÉn b·∫£n th√†nh c√°c chunks c√≥ overlap (Split text into overlapping chunks)
        
        =============================================================================
        LEARNING - CHI·∫æN L∆Ø·ª¢C CHUNKING (CHUNKING STRATEGY)
        =============================================================================
        
        Parameters gi·∫£i th√≠ch (Parameters explained):
        --------------------------------------------
        - chunk_size: S·ªë WORDS (t·ª´) per chunk, KH√îNG ph·∫£i characters!
          * Default: 500 words ‚âà 2-3 paragraphs
          * ƒê·ªß context ƒë·ªÉ c√¢u c√≥ nghƒ©a ho√†n ch·ªânh
          * Kh√¥ng qu√° d√†i ‚Üí v·∫´n trong gi·ªõi h·∫°n embedding model
        
        - overlap: S·ªë words CH·ªíNG L·∫§P gi·ªØa c√°c chunks
          * Default: 50 words ‚âà 2-3 sentences
          * Ensures continuity between chunks
          * Prevents information loss at boundaries
        
        T·∫°i sao ph·∫£i chunk? (Why chunking is necessary?)
        -----------------------------------------------
        1. Technical limit: Gemini max 2048 tokens (~8000 chars)
        2. Precision: Smaller chunks = more accurate retrieval
        3. Performance: Faster embedding v√† search
        4. Context: M·ªói chunk ƒë·ªôc l·∫≠p nh∆∞ng v·∫´n c√≥ nghƒ©a
        
        T·∫°i sao c·∫ßn overlap? (Why overlapping chunks?)
        ----------------------------------------------
        Problem without overlap:
          Chunk 1: "...AI is powerful."
          Chunk 2: "Deep learning helps AI."
          ‚Üí Connection between "powerful" v√† "deep learning" b·ªã m·∫•t!
        
        Solution with overlap:
          Chunk 1: "...AI is powerful. Deep learning helps AI."
          Chunk 2: "Deep learning helps AI. It uses neural networks."
          ‚Üí Context preserved! ‚úÖ
        
        Visual Example (V√≠ d·ª• tr·ª±c quan):
        ---------------------------------
        V·ªõi chunk_size=3, overlap=1:
        
        Text: "The quick brown fox jumps over lazy dog"
        Words: [The, quick, brown, fox, jumps, over, lazy, dog]
                 0    1      2     3     4     5     6     7
        
        Chunks created:
        1. words[0:3]  ‚Üí "The quick brown"
        2. words[2:5]  ‚Üí "brown fox jumps"     <- "brown" overlaps v·ªõi chunk 1
        3. words[4:7]  ‚Üí "jumps over lazy"     <- "jumps" overlaps v·ªõi chunk 2
        4. words[6:8]  ‚Üí "lazy dog"            <- "lazy" overlaps v·ªõi chunk 3
        
        Real-world example (V√≠ d·ª• th·ª±c t·∫ø):
        ----------------------------------
        Document: 5000 words
        chunk_size=500, overlap=50
        
        Step calculation:
        - step = chunk_size - overlap = 500 - 50 = 450 words
        - M·ªói iteration nh·∫£y 450 words (not 500!)
        
        Chunks:
        - Chunk 1: words 0-500     (500 words)
        - Chunk 2: words 450-950   (500 words, overlap 50 with chunk 1)
        - Chunk 3: words 900-1400  (500 words, overlap 50 with chunk 2)
        - ...
        - Total chunks: ~11 chunks
        
        Math behind:
        -----------
        - No overlap: 5000 / 500 = 10 chunks
        - With overlap: 5000 / (500-50) ‚âà 11 chunks
        - Trade-off: Th√™m 10% storage cho better accuracy
        
        Args:
            text: Full document text c·∫ßn chia nh·ªè
            chunk_size: Target s·ªë words per chunk (m·∫∑c ƒë·ªãnh 500)
            overlap: S·ªë words ch·ªìng l·∫•p between chunks (m·∫∑c ƒë·ªãnh 50)
            
        Returns:
            List of text chunks (strings), m·ªói chunk ~chunk_size words
        """
        # Split text into words
        words = text.split()
        chunks = []
        
        # Calculate step size (how many words to advance each time)
        step = chunk_size - overlap
        
        # LEARNING: range(start, stop, step)
        # V√≠ d·ª•: range(0, 100, 50) ‚Üí [0, 50]
        for i in range(0, len(words), step):
            # Get chunk of words
            chunk_words = words[i:i + chunk_size]
            
            # Join back into string
            chunk = ' '.join(chunk_words)
            
            # Only add if chunk has meaningful content
            if len(chunk.strip()) > 0:
                chunks.append(chunk)
        
        print(f"üìÑ Split text into {len(chunks)} chunks")
        return chunks
    
    
    def embed_text(self, text: str) -> List[float]:
        """
        Chuy·ªÉn ƒë·ªïi text th√†nh embedding vector (Convert text to embedding vector)
        
        =============================================================================
        LEARNING - C√ÅCH EMBEDDING HO·∫†T ƒê·ªòNG (HOW EMBEDDING WORKS)
        =============================================================================
        
        Quy tr√¨nh x·ª≠ l√Ω (Processing pipeline):
        --------------------------------------
        Input text: "con m√®o m√†u tr·∫Øng"
        
        Step 1 - Tokenization (T√°ch t·ª´):
          * Gemini chia text th√†nh tokens (sub-word units)
          * Example: "con m√®o" ‚Üí ["con", "m√®o"] ho·∫∑c ["con", " m", "√®o"]
          * T·∫°i sao sub-words? Handle unknown words t·ªët h∆°n
        
        Step 2 - Neural Network Processing:
          * Tokens ƒëi qua multiple layers c·ªßa transformer model
          * M·ªói layer h·ªçc features kh√°c nhau:
            - Layer ƒë·∫ßu: syntax, grammar patterns
            - Layer gi·ªØa: semantic relationships
            - Layer cu·ªëi: high-level concepts
        
        Step 3 - Output Layer (768 neurons):
          * Final layer c√≥ 768 neurons
          * M·ªói neuron output = 1 s·ªë th·ª±c (float)
          * K·∫øt qu·∫£: [0.123, -0.456, 0.789, ..., 0.234]
        
        Output: 768-dimensional vector (768 s·ªë th·ª±c)
        
        T·∫°i sao 768 dimensions? (Why 768?)
        ----------------------------------
        - ƒê·ªß ƒë·ªÉ encode complex semantic information
        - Not too large ‚Üí efficient storage & computation
        - Industry standard (BERT, many models use 768)
        - Each dimension captures m·ªôt aspect c·ªßa meaning
        
        Visual representation (H√¨nh dung):
        ---------------------------------
        Imagine 768-dimensional space (kh√¥ng gian 768 chi·ªÅu):
        
        "con m√®o"     ‚Üí Point A: [0.1, 0.5, -0.3, ..., 0.2]
        "con ch√≥"     ‚Üí Point B: [0.12, 0.48, -0.28, ..., 0.19]  (g·∫ßn A)
        "√¥ t√¥"        ‚Üí Point C: [0.8, -0.2, 0.5, ..., -0.4]     (xa A & B)
        "xe h∆°i"      ‚Üí Point D: [0.79, -0.19, 0.51, ..., -0.39] (g·∫ßn C)
        
        Distance in space = semantic difference (kh√°c bi·ªát nghƒ©a)
        
        The magic behind similarity (Ph√©p m√†u c·ªßa similarity):
        ------------------------------------------------------
        1. Cosine Similarity formula:
           similarity = (A ¬∑ B) / (||A|| √ó ||B||)
           
        2. Range: -1 to 1
           * 1.0 = ho√†n to√†n gi·ªëng nhau (identical meaning)
           * 0.8-0.9 = r·∫•t gi·ªëng (very similar)
           * 0.5-0.7 = c√≥ li√™n quan (related)
           * 0.0 = kh√¥ng li√™n quan (unrelated)
           * -1.0 = tr√°i ng∆∞·ª£c (opposite)
        
        3. Example scores:
           * "h·ªçc Python" vs "h·ªçc l·∫≠p tr√¨nh Python": 0.92 ‚úÖ
           * "h·ªçc Python" vs "Python tutorial": 0.85 ‚úÖ
           * "h·ªçc Python" vs "h·ªçc n·∫•u ƒÉn": 0.12 ‚ùå
        
        Real-world example (V√≠ d·ª• th·ª±c t·∫ø):
        ----------------------------------
        Scenario: User asks "C√°ch c√†i ƒë·∫∑t Python?"
        
        1. Embed question:
           ‚Üí [0.234, -0.567, 0.123, ..., 0.890]
        
        2. Embed all document chunks:
           Chunk 1: "How to install Python on Windows"
           ‚Üí [0.240, -0.560, 0.130, ..., 0.885]  (score: 0.95) ‚úÖ
           
           Chunk 2: "Python list comprehension tutorial"
           ‚Üí [0.100, 0.200, -0.300, ..., 0.400]  (score: 0.45) ‚ùå
           
           Chunk 3: "Installing Python: A beginner's guide"
           ‚Üí [0.238, -0.565, 0.125, ..., 0.888]  (score: 0.93) ‚úÖ
        
        3. Return top-k highest scored chunks
        
        Why embeddings beat keywords (T·∫°i sao t·ªët h∆°n t·ª´ kh√≥a):
        -------------------------------------------------------
        Keyword search:
          Query: "c√†i ƒë·∫∑t Python"
          Misses: "install Python", "Python setup", "getting started Python"
        
        Embedding search:
          Query: "c√†i ƒë·∫∑t Python" 
          Finds: ALL above! Because semantic meaning is captured.
        
        Args:
            text: Text c·∫ßn chuy·ªÉn th√†nh embedding (c√≥ th·ªÉ l√† c√¢u, ƒëo·∫°n, chunk)
            
        Returns:
            768-dimensional vector (list of 768 floats)
            Example: [0.123, -0.456, ..., 0.789]
        """
        try:
            # Call Gemini embedding API
            # LEARNING: embed_content() is specifically for embeddings
            # NOTE: API updated - use 'contents' instead of 'content'
            response = self.client.models.embed_content(
                model=self.model,
                contents=text  # Changed from 'content' to 'contents'
            )
            
            # Extract embedding vector
            # LEARNING: Response structure t·ª´ Gemini
            embedding = response.embeddings[0].values
            
            # Verify dimensions
            if len(embedding) != 768:
                raise ValueError(f"Expected 768 dimensions, got {len(embedding)}")
            
            return embedding
            
        except Exception as e:
            print(f"‚ùå Embedding error: {str(e)}")
            raise
    
    
    def embed_chunks(self, chunks: List[str], batch_size: int = 5) -> List[Dict[str, Any]]:
        """
        Embed multiple text chunks with rate limiting
        
        LEARNING - BATCH PROCESSING:
        ============================
        T·∫°i sao batch?
        - API c√≥ rate limits (requests per minute)
        - Batch = group nhi·ªÅu chunks, process together
        - Efficient h∆°n l√† call API t·ª´ng chunk
        
        Rate limiting:
        - Free tier: 60 requests/minute
        - Batch 5 chunks = safer, avoid hitting limits
        - Sleep between batches ƒë·ªÉ respect limits
        
        Args:
            chunks: List of text chunks to embed
            batch_size: Number of chunks per batch
            
        Returns:
            List of dicts with:
                - chunk_index: Position in original list
                - text: The chunk text
                - embedding: 768d vector
                - metadata: Additional info (length, etc.)
        """
        results = []
        
        print(f"üîÑ Starting to embed {len(chunks)} chunks...")
        
        # Process in batches
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(chunks) + batch_size - 1) // batch_size
            
            print(f"üì¶ Processing batch {batch_num}/{total_batches} ({len(batch)} chunks)")
            
            # Embed each chunk in batch
            for j, chunk in enumerate(batch):
                chunk_index = i + j
                
                try:
                    # Get embedding
                    embedding = self.embed_text(chunk)
                    
                    # Store result with metadata
                    results.append({
                        "chunk_index": chunk_index,
                        "text": chunk,
                        "embedding": embedding,
                        "metadata": {
                            "length": len(chunk),
                            "words": len(chunk.split()),
                        }
                    })
                    
                    print(f"  ‚úì Embedded chunk {chunk_index + 1}")
                    
                except Exception as e:
                    print(f"  ‚úó Failed chunk {chunk_index + 1}: {str(e)}")
                    continue
            
            # Rate limiting: wait between batches
            # LEARNING: Avoid hitting API rate limits
            if i + batch_size < len(chunks):
                wait_time = 2  # seconds
                print(f"‚è≥ Waiting {wait_time}s before next batch...")
                time.sleep(wait_time)
        
        print(f"‚úÖ Successfully embedded {len(results)}/{len(chunks)} chunks")
        return results
    
    
    def process_document(self, text: str) -> List[Dict[str, Any]]:
        """
        Complete pipeline: chunk text ‚Üí embed all chunks
        
        LEARNING - FULL PIPELINE:
        ========================
        This is the main function you'll call for a document
        
        Steps:
        1. Receive full document text
        2. Split into manageable chunks
        3. Embed each chunk
        4. Return all embeddings with metadata
        
        Usage:
            text = "Your long document text here..."
            results = service.process_document(text)
            # Now save results to vector database (ChromaDB)
        
        Args:
            text: Full document text
            
        Returns:
            List of embeddings with text and metadata
        """
        print("\n" + "="*60)
        print("üìö DOCUMENT EMBEDDING PIPELINE")
        print("="*60)
        
        # Step 1: Chunk
        print("\nüìÑ Step 1: Chunking document...")
        chunks = self.chunk_text(text, chunk_size=500, overlap=50)
        
        # Step 2: Embed
        print(f"\nüéØ Step 2: Embedding {len(chunks)} chunks...")
        embeddings = self.embed_chunks(chunks, batch_size=5)
        
        print("\n" + "="*60)
        print(f"‚úÖ COMPLETE! Processed {len(embeddings)} embeddings")
        print("="*60 + "\n")
        
        return embeddings


# LEARNING - HOW TO USE THIS SERVICE:
# ===================================
"""
Example usage:

# 1. Initialize service
service = EmbeddingService()

# 2. Your document text
document_text = '''
Your long PDF or DOCX content here.
This could be many pages of text.
The service will automatically chunk and embed it.
'''

# 3. Process document
embeddings = service.process_document(document_text)

# 4. Each embedding looks like:
# {
#     "chunk_index": 0,
#     "text": "Your first chunk of text here...",
#     "embedding": [0.1, 0.2, ..., 0.8],  # 768 numbers
#     "metadata": {
#         "length": 2543,
#         "words": 450
#     }
# }

# 5. Next step: Save to ChromaDB for vector search!
"""
