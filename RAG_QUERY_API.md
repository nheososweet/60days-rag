# RAG Query API Documentation

H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ RAG (Retrieval-Augmented Generation) Query API

---

## üìö Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [API Endpoints](#api-endpoints)
4. [Usage Examples](#usage-examples)
5. [Testing Guide](#testing-guide)
6. [Best Practices](#best-practices)
7. [Troubleshooting](#troubleshooting)

---

## üéØ Overview

### RAG l√† g√¨?

**RAG (Retrieval-Augmented Generation)** l√† k·ªπ thu·∫≠t k·∫øt h·ª£p:
- **Retrieval**: T√¨m ki·∫øm th√¥ng tin t·ª´ documents
- **Augmented**: TƒÉng c∆∞·ªùng context cho AI
- **Generation**: AI sinh ra c√¢u tr·∫£ l·ªùi d·ª±a tr√™n context

### Workflow

```
User Question
    ‚Üì
Embed Question (768d vector)
    ‚Üì
Search Similar Chunks (Vector DB)
    ‚Üì
Build Context (Top-K chunks)
    ‚Üì
Create Prompt (Instructions + Context + Question)
    ‚Üì
Call Gemini AI
    ‚Üì
Generate Answer + Sources
```

### So s√°nh: Chat thu·∫ßn vs RAG

| Feature | Chat Thu·∫ßn | RAG |
|---------|------------|-----|
| **Knowledge** | General (training data) | Specific (your documents) |
| **Up-to-date** | ‚ùå Outdated | ‚úÖ Current (your data) |
| **Hallucination** | ‚ö†Ô∏è Possible | ‚úÖ Grounded in context |
| **Source** | ‚ùå No citations | ‚úÖ Shows sources |
| **Use Case** | General Q&A | Domain-specific Q&A |

---

## üèóÔ∏è Architecture

### Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RAG API        ‚îÇ
‚îÇ  (rag.py)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îú‚îÄ‚ñ∫ EmbeddingService (text-embedding-004)
     ‚îÇ   ‚îî‚îÄ Convert text ‚Üí 768d vector
     ‚îÇ
     ‚îú‚îÄ‚ñ∫ VectorDBService (ChromaDB)
     ‚îÇ   ‚îî‚îÄ Store & search embeddings
     ‚îÇ
     ‚îî‚îÄ‚ñ∫ GeminiService (gemini-2.0-flash-exp)
         ‚îî‚îÄ Generate answers from context
```

### Data Flow

```
Documents (PDF, TXT)
    ‚Üì (uploaded via /api/documents/upload)
Chunks (500-1000 chars)
    ‚Üì (embedded via text-embedding-004)
Vectors (768 dimensions)
    ‚Üì (stored in ChromaDB)
Ready for RAG queries!
```

---

## üîå API Endpoints

### 1. POST /api/rag/query

H·ªèi c√¢u h·ªèi d·ª±a tr√™n documents ƒë√£ upload.

#### Request Body

```json
{
  "question": "Giyu Tomioka l√† ai?",
  "n_results": 5,
  "document_id": null,
  "include_context": false
}
```

**Parameters:**

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `question` | string | ‚úÖ Yes | - | C√¢u h·ªèi c·ªßa user |
| `n_results` | integer | ‚ùå No | 5 | S·ªë chunks mu·ªën l·∫•y (1-20) |
| `document_id` | string | ‚ùå No | null | Filter theo document c·ª• th·ªÉ |
| `include_context` | boolean | ‚ùå No | false | Return context ƒë√£ d√πng |

**N_results Guide:**
- **1-3**: Fast, focused (c√≥ th·ªÉ miss info)
- **5-10**: Balanced, recommended ‚úÖ
- **15-20**: Comprehensive (expensive, may confuse AI)

#### Response

```json
{
  "success": true,
  "question": "Giyu Tomioka l√† ai?",
  "answer": "Giyu Tomioka l√† Tr·ª• N∆∞·ªõc (Water Hashira) trong S√°t Qu·ª∑ ƒê·ªôi...",
  "sources": [
    {
      "chunk_id": "chunk_123",
      "text": "Full chunk text here...",
      "text_preview": "Preview of chunk...",
      "distance": 0.234,
      "similarity": 0.766,
      "metadata": {
        "filename": "characters.txt",
        "chunk_index": 5,
        "document_id": "doc_abc"
      }
    }
  ],
  "context_used": "Full context string if include_context=true",
  "metadata": {
    "chunks_used": 5,
    "total_chunks_available": 150,
    "context_length": 2500,
    "answer_length": 350,
    "processing_time_seconds": 2.5,
    "model": "gemini-2.0-flash-exp",
    "embedding_model": "text-embedding-004"
  }
}
```

#### Error Responses

**404 - No Documents**
```json
{
  "detail": "No documents found. Please upload and embed documents first."
}
```

**404 - No Matches**
```json
{
  "detail": "No relevant information found for your question."
}
```

**500 - AI Service Error**
```json
{
  "detail": "AI service error: [error message]"
}
```

---

### 2. GET /api/rag/stats

Ki·ªÉm tra tr·∫°ng th√°i RAG system.

#### Request

```bash
GET /api/rag/stats
```

#### Response

```json
{
  "ready": true,
  "total_documents": 3,
  "total_chunks": 150,
  "status": "ready",
  "message": "RAG system ready with 3 documents and 150 chunks",
  "collection_name": "documents",
  "embedding_model": "text-embedding-004",
  "chat_model": "gemini-2.0-flash-exp"
}
```

**Status Values:**
- `"ready"`: System c√≥ data, ready for queries ‚úÖ
- `"no_data"`: Ch∆∞a c√≥ documents, c·∫ßn upload ‚ö†Ô∏è
- `"error"`: Database error ‚ùå

---

## üí° Usage Examples

### Example 1: Basic Query

```bash
curl -X POST http://localhost:3201/api/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Giyu Tomioka l√† ai?",
    "n_results": 5
  }'
```

### Example 2: Query v·ªõi Context

```bash
curl -X POST http://localhost:3201/api/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Shinobu Kocho c√≥ t√≠nh c√°ch nh∆∞ th·∫ø n√†o?",
    "n_results": 3,
    "include_context": true
  }'
```

### Example 3: Filter by Document

```bash
curl -X POST http://localhost:3201/api/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Who are the main characters?",
    "n_results": 10,
    "document_id": "doc_abc123"
  }'
```

### Example 4: Python Client

```python
import requests

response = requests.post(
    "http://localhost:3201/api/rag/query",
    json={
        "question": "Giyu Tomioka l√† ai?",
        "n_results": 5,
        "include_context": False
    }
)

result = response.json()
print(f"Answer: {result['answer']}")
print(f"Sources: {len(result['sources'])} chunks")
```

### Example 5: Check Stats

```bash
curl http://localhost:3201/api/rag/stats
```

---

## üß™ Testing Guide

### Prerequisites

1. **Server running**
   ```bash
   cd 60days-rag
   python main.py
   ```

2. **Documents uploaded & embedded**
   ```bash
   python test_document_api.py
   ```

### Run RAG Tests

```bash
python test_rag_query.py
```

**Test Script Features:**
- ‚úÖ Check RAG stats
- ‚úÖ Test multiple queries
- ‚úÖ Show answers + sources
- ‚úÖ Display metadata & timing
- ‚úÖ Formatted output

### Manual Testing

**Step 1: Check stats**
```bash
curl http://localhost:3201/api/rag/stats
```

**Step 2: Test query**
```bash
curl -X POST http://localhost:3201/api/rag/query \
  -H "Content-Type: application/json" \
  -d '{"question":"Giyu Tomioka l√† ai?","n_results":5}'
```

**Step 3: Check server logs**
- Server console shows detailed workflow:
  - ‚úÖ Step 1: Checking DB
  - üéØ Step 2: Embedding question
  - üîç Step 3: Searching chunks
  - üìù Step 4: Building context
  - üí≠ Step 5: Creating prompt
  - ü§ñ Step 6: Calling Gemini
  - üì¶ Step 7: Formatting response

---

## üéì Best Practices

### 1. Choosing N_Results

```python
# Small query (specific info)
n_results = 3  # Fast, focused

# Medium query (balanced)
n_results = 5-10  # Recommended ‚úÖ

# Complex query (comprehensive)
n_results = 15-20  # Expensive, but thorough
```

### 2. Prompt Engineering Principles

Our RAG implementation uses:

```
Instructions (how to answer)
    ‚Üì
Context (relevant chunks)
    ‚Üì
Question (user's query)
    ‚Üì
Output Format (structure)
```

**Key principles:**
- ‚úÖ "Answer ONLY based on context" ‚Üí No hallucination
- ‚úÖ "Cite sources [Source 1]" ‚Üí Traceability
- ‚úÖ "Same language as question" ‚Üí Vietnamese/English flexibility
- ‚úÖ "Say 'no info' if context insufficient" ‚Üí Honesty

### 3. Error Handling

```python
try:
    response = requests.post(url, json=data)
    response.raise_for_status()  # Raise for 4xx/5xx
    result = response.json()
except requests.exceptions.HTTPError as e:
    if e.response.status_code == 404:
        print("No documents or no matches")
    elif e.response.status_code == 500:
        print("Server error")
except Exception as e:
    print(f"Unexpected error: {e}")
```

### 4. Performance Optimization

| Metric | Impact | Optimization |
|--------|--------|--------------|
| **Embedding** | ~200ms | Batch questions if many |
| **Vector Search** | ~50ms | Use filters (document_id) |
| **Gemini API** | ~2s | Reduce n_results if slow |
| **Total** | ~2.5s | Acceptable for most use cases |

### 5. Context Window Management

**Gemini Limit:** 30,720 tokens (~100k characters)

```python
# Safe approach
n_results = 5  # ~2,500 chars context
# Rarely hits limit

# Aggressive approach  
n_results = 20  # ~10,000 chars context
# Risk hitting limit if chunks are large
```

---

## üîß Troubleshooting

### Issue 1: "No documents found"

**Problem:** RAG query returns 404

**Solutions:**
1. Check stats: `curl http://localhost:3201/api/rag/stats`
2. Upload documents: `python test_document_api.py`
3. Verify ChromaDB: Check `chroma_db/` folder exists
4. Check server logs for errors

### Issue 2: "No relevant information found"

**Problem:** Query returns no matches

**Causes:**
- Question kh√¥ng li√™n quan ƒë·∫øn documents
- Embeddings ch∆∞a capture semantic meaning
- Documents qu√° √≠t

**Solutions:**
1. Rephrase question (different words, same meaning)
2. Increase n_results (try 10-15)
3. Check if documents contain relevant info
4. Use `include_context=true` to debug

### Issue 3: Poor answer quality

**Problem:** AI answer kh√¥ng ƒë√∫ng ho·∫∑c vague

**Causes:**
- n_results qu√° th·∫•p ‚Üí missed important info
- n_results qu√° cao ‚Üí confused by too much context
- Chunks kh√¥ng ch·ª©a ƒë·ªß context

**Solutions:**
1. Tune n_results (start with 5, adjust)
2. Improve document chunking strategy
3. Check similarity scores (should be >0.7)
4. Review context used (`include_context=true`)

### Issue 4: Slow response

**Problem:** Query takes >5 seconds

**Causes:**
- Large n_results
- Many chunks in DB
- Gemini API latency
- Network issues

**Solutions:**
1. Reduce n_results (5 instead of 20)
2. Use document_id filter
3. Check Gemini API status
4. Monitor server logs for bottlenecks

### Issue 5: Server errors (500)

**Problem:** Internal server error

**Causes:**
- Gemini API key invalid/expired
- ChromaDB database corrupted
- Out of memory
- Code bugs

**Solutions:**
1. Check `.env` file (GOOGLE_API_KEY)
2. Test Gemini: `python test_client.py`
3. Restart server
4. Check server console for stack trace
5. Delete `chroma_db/` and re-upload documents

---

## üìù Code Structure

### File: `app/api/rag.py`

```python
# Main components:
1. Pydantic Models (RAGQueryRequest, RAGQueryResponse)
2. RAG Query Endpoint (7-step workflow)
3. Stats Endpoint (system health check)

# Services used:
- EmbeddingService: text ‚Üí vector
- VectorDBService: store & search vectors
- GeminiService: generate answers
```

### Detailed Workflow

```python
@router.post("/query")
async def rag_query(request: RAGQueryRequest):
    # Step 1: Validate (DB has documents?)
    stats = vector_db.get_collection_stats()
    
    # Step 2: Embed question
    question_embedding = embedding_service.embed_text(request.question)
    
    # Step 3: Search similar chunks
    search_results = vector_db.search(
        query_embedding=question_embedding,
        n_results=request.n_results
    )
    
    # Step 4: Build context
    context = "\n\n---\n\n".join([
        f"[Source {i}] {chunk['text']}"
        for i, chunk in enumerate(search_results['results'], 1)
    ])
    
    # Step 5: Create prompt
    prompt = f"""Answer based ONLY on context:
    
CONTEXT:
{context}

QUESTION: {request.question}
"""
    
    # Step 6: Generate answer
    answer = gemini_service.chat(prompt)
    
    # Step 7: Format response
    return RAGQueryResponse(
        success=True,
        answer=answer,
        sources=[...],
        metadata={...}
    )
```

---

## üöÄ Next Steps

### Phase 1: Basic RAG ‚úÖ (Current)
- ‚úÖ RAG query endpoint
- ‚úÖ Stats endpoint
- ‚úÖ Source citations
- ‚úÖ Metadata tracking

### Phase 2: Advanced RAG (Next)
- ‚è≥ Multi-query expansion
- ‚è≥ Re-ranking results
- ‚è≥ Streaming responses
- ‚è≥ Query history

### Phase 3: Agentic RAG (Future)
- ‚è≥ LangGraph agents
- ‚è≥ Multi-step reasoning
- ‚è≥ Tool calling
- ‚è≥ Self-correction

---

## üìö Resources

- [RAG Overview](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [ChromaDB Docs](https://docs.trychroma.com/)
- [Gemini API](https://ai.google.dev/gemini-api/docs)
- [FastAPI Best Practices](https://fastapi.tiangolo.com/tutorial/)

---

## üôè Credits

Project: 60 Days RAG Learning
Author: Learning FastAPI + RAG + LangChain
Framework: FastAPI + ChromaDB + Google Gemini

---

**Happy RAG Querying! üöÄ**
