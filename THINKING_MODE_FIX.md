# üîß Thinking Mode Fix - Chi ti·∫øt c√°c thay ƒë·ªïi

## ‚ùå V·∫•n ƒë·ªÅ ban ƒë·∫ßu

Khi b·∫°n call API v·ªõi `enable_thinking: true`, kh√¥ng th·∫•y response c√≥ thinking content:

```bash
curl -X 'POST' \
  'http://127.0.0.1:3201/qwen/chat/stream' \
  -H 'Content-Type: application/json' \
  -d '{
  "message": "Explain what is RAG in AI?",
  "stream": true,
  "temperature": 0.7,
  "enable_thinking": true
}'
```

**K·∫øt qu·∫£**: Ch·ªâ th·∫•y `type: "content"`, kh√¥ng c√≥ `type: "thinking"`

## üîç Root Cause Analysis

### 1. **ChatRequest model thi·∫øu field `enable_thinking`**

- File: `app/models/schemas.py`
- Model ch·ªâ c√≥: `message`, `temperature`, `max_tokens`, `stream`, `system_prompt`
- **Thi·∫øu**: `enable_thinking` v√† `context`

### 2. **API routes kh√¥ng pass parameter**

- File: `app/api/qwen.py`
- C·∫£ 2 endpoints (`/chat` v√† `/chat/stream`) ƒë·ªÅu kh√¥ng pass `enable_thinking` v√†o service
- Service nh·∫≠n `enable_thinking=False` (default value)

## ‚úÖ C√°c fixes ƒë√£ th·ª±c hi·ªán

### Fix 1: Th√™m fields v√†o ChatRequest model

**File**: `app/models/schemas.py`

```python
class ChatRequest(BaseModel):
    """Request model for chat endpoint."""
    message: str = Field(...)
    conversation_id: Optional[str] = Field(None, ...)
    model: Optional[str] = Field(None, ...)
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0, ...)
    max_tokens: Optional[int] = Field(None, gt=0, ...)
    stream: bool = Field(False, ...)
    system_prompt: Optional[str] = Field(None, ...)

    # ‚úÖ ADDED: Support thinking mode
    enable_thinking: Optional[bool] = Field(
        False,
        description="Enable thinking mode - model shows reasoning process in <think> tags (Qwen only)"
    )

    # ‚úÖ ADDED: Support RAG context injection
    context: Optional[str] = Field(
        None,
        description="Additional context to inject into prompt (used for RAG)"
    )
```

### Fix 2: Pass parameters trong non-streaming endpoint

**File**: `app/api/qwen.py` - `/qwen/chat` endpoint

```python
result = await qwen_service.generate_response(
    message=request.message,
    temperature=request.temperature,
    max_tokens=request.max_tokens,
    conversation_id=request.conversation_id,
    system_prompt=request.system_prompt,
    context=request.context,                        # ‚úÖ ADDED
    enable_thinking=request.enable_thinking or False # ‚úÖ ADDED
)
```

### Fix 3: Pass parameters trong streaming endpoint

**File**: `app/api/qwen.py` - `/qwen/chat/stream` endpoint

```python
async for chunk_data in qwen_service.generate_stream_response(
    message=request.message,
    temperature=request.temperature,
    max_tokens=request.max_tokens,
    conversation_id=request.conversation_id,
    system_prompt=request.system_prompt,
    context=request.context,                        # ‚úÖ ADDED
    enable_thinking=request.enable_thinking or False # ‚úÖ ADDED
):
    # ... yield chunks
```

## üß™ Test Cases

### Test 1: Streaming v·ªõi thinking mode

```bash
curl -X POST http://127.0.0.1:3201/qwen/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is 2+2? Think step by step.",
    "temperature": 0.7,
    "enable_thinking": true,
    "system_prompt": "You are a math tutor. Show your reasoning."
  }'
```

**Expected output**:

```
data: {"type":"thinking","thinking_content":"Let me think... 2+2 means...","chunk":"","done":false}
data: {"type":"content","chunk":"The answer is 4","done":false}
data: {"chunk":"","done":true}
```

### Test 2: Non-streaming v·ªõi thinking mode

```bash
curl -X POST http://127.0.0.1:3201/qwen/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is 2+2? Think step by step.",
    "enable_thinking": true
  }'
```

**Expected output**:

```json
{
  "response": "The answer is 4",
  "thinking_content": "Let me think... 2+2 means adding 2 and 2...",
  "conversation_id": "qwen_conv_abc123",
  "model": "Qwen/Qwen3-0.6B",
  "enable_thinking": true,
  "usage": {...}
}
```

### Test 3: Python test scripts

```bash
# Quick test
python quick_test_thinking.py

# Comprehensive test suite
python test_thinking.py
```

## üìä Response Format chi ti·∫øt

### Streaming Response Chunks

#### Type 1: Thinking chunk

```json
{
  "type": "thinking",
  "thinking_content": "Let me analyze this step by step...",
  "chunk": "",
  "done": false,
  "conversation_id": "qwen_conv_123"
}
```

#### Type 2: Content chunk

```json
{
  "type": "content",
  "chunk": "The answer is",
  "done": false,
  "conversation_id": "qwen_conv_123"
}
```

#### Type 3: Finish chunk

```json
{
  "type": "finish",
  "finish_reason": "stop",
  "chunk": "",
  "done": true,
  "conversation_id": "qwen_conv_123"
}
```

#### Type 4: Error chunk

```json
{
  "type": "error",
  "chunk": "Connection error: ...",
  "done": true,
  "error": true,
  "conversation_id": "qwen_conv_123"
}
```

### Non-Streaming Response

```json
{
  "response": "Final answer without <think> tags",
  "thinking_content": "Reasoning process extracted from <think> tags",
  "conversation_id": "qwen_conv_123",
  "model": "Qwen/Qwen3-0.6B",
  "enable_thinking": true,
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 50,
    "total_tokens": 60
  }
}
```

## ‚ö†Ô∏è Important Notes v·ªÅ Thinking Mode

### 1. **Model behavior**

- Qwen3-0.6B **kh√¥ng ph·∫£i l√∫c n√†o** c≈©ng generate `<think>` tags
- T·ª∑ l·ªá xu·∫•t hi·ªán thinking tags ph·ª• thu·ªôc v√†o:
  - Prompt quality ("Think step by step", "Show your reasoning")
  - System prompt ("You must show thinking process")
  - Temperature (th·∫•p h∆°n = consistent h∆°n)
  - Question complexity (complex questions ‚Üí more thinking)

### 2. **Better thinking support**

Models c√≥ thinking mode t·ªët h∆°n:

- **Qwen2.5-7B-Instruct**: Better reasoning, more consistent
- **QwQ-32B-Preview**: Specialized thinking model
- **Qwen2.5-14B/32B**: Advanced reasoning capabilities

### 3. **Prompt engineering tips**

```python
# ‚ùå Weak prompt
"What is RAG?"

# ‚úÖ Strong prompt
"Explain what is RAG. Think carefully step by step before answering."

# ‚úÖ Best prompt with system instruction
system_prompt = "You are an AI expert who always shows reasoning process in <think> tags before answering."
message = "Explain RAG in simple terms."
```

### 4. **Debug thinking mode**

Check if model actually generates `<think>` tags:

```python
# Enable vLLM debug logging
# Trong vLLM server, check raw output
# N·∫øu kh√¥ng c√≥ <think> trong raw output ‚Üí model kh√¥ng generate
# N·∫øu c√≥ <think> nh∆∞ng kh√¥ng parse ‚Üí bug trong parser
```

## üöÄ Next Steps

1. **Test v·ªõi current model**:

   ```bash
   python quick_test_thinking.py
   ```

2. **N·∫øu kh√¥ng c√≥ thinking content**:

   - Try stronger prompts
   - Check vLLM server logs
   - Consider upgrading model

3. **Optional: Upgrade model** (n·∫øu c·∫ßn better thinking):

   ```bash
   # Stop current vLLM
   # Start with bigger model
   python -m vllm.entrypoints.openai.api_server \
     --model Qwen/Qwen2.5-7B-Instruct \
     --port 8000

   # Update config.py
   QWEN_MODEL = "Qwen/Qwen2.5-7B-Instruct"
   ```

4. **Move to RAG integration**:
   - Thinking mode s·∫Ω r·∫•t h·ªØu √≠ch cho RAG
   - C√≥ th·ªÉ th·∫•y model reasoning v·ªÅ retrieved context
   - Debug why model gives certain answers

## üìÅ Files Changed

1. ‚úÖ `app/models/schemas.py` - Added `enable_thinking` v√† `context` fields
2. ‚úÖ `app/api/qwen.py` - Pass parameters to service (both endpoints)
3. ‚úÖ `app/services/qwen_service.py` - Already has thinking support (unchanged)
4. ‚úÖ `test_thinking.py` - Comprehensive test suite
5. ‚úÖ `quick_test_thinking.py` - Quick verification script

## üéØ Summary

**Before**: `enable_thinking: true` trong request ‚Üí b·ªã ignore ‚Üí no thinking output

**After**: `enable_thinking: true` ‚Üí passed to service ‚Üí thinking parser activated ‚Üí thinking content in response

**Result**: Thinking mode **should work** n·∫øu model generates `<think>` tags. N·∫øu kh√¥ng th·∫•y thinking, ƒë√≥ l√† model behavior, kh√¥ng ph·∫£i bug.
